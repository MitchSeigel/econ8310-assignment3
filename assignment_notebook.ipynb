{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e649671-64ae-4692-a381-33974ffa666a",
      "metadata": {
        "id": "8e649671-64ae-4692-a381-33974ffa666a"
      },
      "source": [
        "# Assignment 3\n",
        "## Econ 8310 - Business Forecasting\n",
        "\n",
        "For homework assignment 3, you will work with [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist), a more fancier data set.\n",
        "\n",
        "- You must create a custom data loader as described in the first week of neural network lectures [2 points]\n",
        "    - You will NOT receive credit for this if you use the pytorch prebuilt loader for Fashion MNIST!\n",
        "- You must create a working and trained neural network using only pytorch [2 points]\n",
        "- You must store your weights and create an import script so that I can evaluate your model without training it [2 points]\n",
        "\n",
        "Highest accuracy score gets some extra credit!\n",
        "\n",
        "Submit your forked repository URL on Canvas! :) I'll be manually grading this assignment.\n",
        "\n",
        "Some checks you can make on your own:\n",
        "- Did you manually process the data or use a prebuilt loader (see above)?\n",
        "- Does your script train a neural network on the assigned data?\n",
        "- Did your script save your model?\n",
        "- Do you have separate code to import your model for use after training?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import plotly.express as px\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import urllib.request\n",
        "import gzip\n",
        "import numpy as np\n",
        "import struct\n",
        "\n",
        "# Custom dataloader for the Fashion-MNIST dataset\n",
        "class FashionMNIST(Dataset):\n",
        "    def __init__(self, train_images_url, train_labels_url, test_images_url, test_labels_url, train=True):\n",
        "        self.train = train\n",
        "        self.unzip(train_images_url, train_labels_url, test_images_url, test_labels_url)\n",
        "\n",
        "    # Credit to Google and its AI search assistant as my source for this section on how to unzip and use the .gz files for this custom dataloader\n",
        "    def unzip(self, train_images_url, train_labels_url, test_images_url, test_labels_url):\n",
        "        with urllib.request.urlopen(train_images_url) as response, gzip.GzipFile(fileobj=response) as f:\n",
        "            magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
        "            self.train_images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows, cols)\n",
        "\n",
        "        with urllib.request.urlopen(train_labels_url) as response, gzip.GzipFile(fileobj=response) as f:\n",
        "            magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
        "            self.train_labels = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_labels)\n",
        "\n",
        "        with urllib.request.urlopen(test_images_url) as response, gzip.GzipFile(fileobj=response) as f:\n",
        "            magic, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
        "            self.test_images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows, cols)\n",
        "\n",
        "        with urllib.request.urlopen(test_labels_url) as response, gzip.GzipFile(fileobj=response) as f:\n",
        "            magic, num_labels = struct.unpack(\">II\", f.read(8))\n",
        "            self.test_labels = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_labels) if self.train else len(self.test_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.train:\n",
        "            image = torch.tensor(self.train_images[idx], dtype=torch.float32).unsqueeze(0)\n",
        "            label = torch.tensor(self.train_labels[idx], dtype=torch.long)\n",
        "        else:\n",
        "            image = torch.tensor(self.test_images[idx], dtype=torch.float32).unsqueeze(0)\n",
        "            label = torch.tensor(self.test_labels[idx], dtype=torch.long)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# URLs for training\n",
        "train_images_url = 'https://github.com/zalandoresearch/fashion-mnist/raw/b2617bb6d3ffa2e429640350f613e3291e10b141/data/fashion/train-images-idx3-ubyte.gz'\n",
        "train_labels_url = 'https://github.com/zalandoresearch/fashion-mnist/raw/b2617bb6d3ffa2e429640350f613e3291e10b141/data/fashion/train-labels-idx1-ubyte.gz'\n",
        "\n",
        "# URLs for testing\n",
        "test_images_url = 'https://github.com/zalandoresearch/fashion-mnist/raw/b2617bb6d3ffa2e429640350f613e3291e10b141/data/fashion/t10k-images-idx3-ubyte.gz'\n",
        "test_labels_url = 'https://github.com/zalandoresearch/fashion-mnist/raw/b2617bb6d3ffa2e429640350f613e3291e10b141/data/fashion/t10k-labels-idx1-ubyte.gz'\n",
        "\n",
        "# Create an instances for both the training dataset and testing dataset\n",
        "train_dataset = FashionMNIST(train_images_url, train_labels_url, test_images_url, test_labels_url, train=True)\n",
        "test_dataset = FashionMNIST(train_images_url, train_labels_url, test_images_url, test_labels_url, train=False)\n",
        "\n",
        "# Create the dataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "# len(train_dataset)\n",
        "# len(test_dataset)\n",
        "\n",
        "# idx=2\n",
        "# print(f\"This image is labeled a {train_dataset.__getitemTrain__(idx)[1]}\")\n",
        "# px.imshow(train_dataset.__getitemTrain__(idx)[0].reshape(28, 28))\n",
        "\n",
        "# idx=2\n",
        "# print(f\"This image is labeled a {train_dataset.__getitemTest__(idx)[1]}\")\n",
        "# px.imshow(test_dataset.__getitemTest__(idx)[0].reshape(28, 28))"
      ],
      "metadata": {
        "id": "NhhWfz20vHX8"
      },
      "id": "NhhWfz20vHX8",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(FashionNet, self).__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "\n",
        "      self.linear_relu_model = nn.Sequential(\n",
        "            nn.LazyLinear(10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      output = self.linear_relu_model(x)\n",
        "      return output\n",
        "\n",
        "model = FashionNet()"
      ],
      "metadata": {
        "id": "tLRfsUozIuiU"
      },
      "id": "tLRfsUozIuiU",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some training parameters\n",
        "learning_rate = 1e-2\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "# Define our loss function\n",
        "#   This one works for multiclass problems\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Build our optimizer with the parameters from\n",
        "#   the model we defined, and the learning rate\n",
        "#   that we picked\n",
        "optimizer = torch.optim.SGD(model.parameters(),\n",
        "     lr=learning_rate)"
      ],
      "metadata": {
        "id": "B6R_guU6YHTZ"
      },
      "id": "B6R_guU6YHTZ",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode\n",
        "    # important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    # Loop over batches via the dataloader\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation and looking for improved gradients\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Zeroing out the gradient (otherwise they are summed)\n",
        "        #   in preparation for next round\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print progress update every few loops\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "d_H6ti9cZA9_"
      },
      "id": "d_H6ti9cZA9_",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode\n",
        "    # important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures\n",
        "    # that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations\n",
        "    # and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Printing some output after a testing round\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "qMxUZJwzZ55H"
      },
      "id": "qMxUZJwzZ55H",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGAa9C_4a6FZ",
        "outputId": "31891e46-562c-4054-991d-cf3051522602"
      },
      "id": "iGAa9C_4a6FZ",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 116.636414  [   64/60000]\n",
            "loss: 3874.153076  [  704/60000]\n",
            "loss: 6645.219727  [ 1344/60000]\n",
            "loss: 1359.909302  [ 1984/60000]\n",
            "loss: 4790.759766  [ 2624/60000]\n",
            "loss: 2746.125000  [ 3264/60000]\n",
            "loss: 1421.954468  [ 3904/60000]\n",
            "loss: 2783.031494  [ 4544/60000]\n",
            "loss: 3056.596436  [ 5184/60000]\n",
            "loss: 4960.110840  [ 5824/60000]\n",
            "loss: 700.722656  [ 6464/60000]\n",
            "loss: 1035.757324  [ 7104/60000]\n",
            "loss: 2515.841309  [ 7744/60000]\n",
            "loss: 3583.726562  [ 8384/60000]\n",
            "loss: 2051.877930  [ 9024/60000]\n",
            "loss: 5048.052246  [ 9664/60000]\n",
            "loss: 761.846741  [10304/60000]\n",
            "loss: 3646.296631  [10944/60000]\n",
            "loss: 1610.748657  [11584/60000]\n",
            "loss: 6676.822754  [12224/60000]\n",
            "loss: 1157.899170  [12864/60000]\n",
            "loss: 2123.333984  [13504/60000]\n",
            "loss: 1851.886353  [14144/60000]\n",
            "loss: 827.585754  [14784/60000]\n",
            "loss: 1341.860352  [15424/60000]\n",
            "loss: 1408.541504  [16064/60000]\n",
            "loss: 1297.861084  [16704/60000]\n",
            "loss: 1588.118774  [17344/60000]\n",
            "loss: 1342.023193  [17984/60000]\n",
            "loss: 1091.092651  [18624/60000]\n",
            "loss: 1579.497192  [19264/60000]\n",
            "loss: 3083.205078  [19904/60000]\n",
            "loss: 1535.445679  [20544/60000]\n",
            "loss: 680.363647  [21184/60000]\n",
            "loss: 3658.895508  [21824/60000]\n",
            "loss: 1461.611572  [22464/60000]\n",
            "loss: 1357.892334  [23104/60000]\n",
            "loss: 1048.588257  [23744/60000]\n",
            "loss: 2487.545410  [24384/60000]\n",
            "loss: 1541.594238  [25024/60000]\n",
            "loss: 801.263123  [25664/60000]\n",
            "loss: 2168.723633  [26304/60000]\n",
            "loss: 1349.557861  [26944/60000]\n",
            "loss: 2446.614990  [27584/60000]\n",
            "loss: 594.001221  [28224/60000]\n",
            "loss: 1921.019897  [28864/60000]\n",
            "loss: 1453.104492  [29504/60000]\n",
            "loss: 2802.528809  [30144/60000]\n",
            "loss: 1263.095947  [30784/60000]\n",
            "loss: 587.073547  [31424/60000]\n",
            "loss: 827.104370  [32064/60000]\n",
            "loss: 1561.372681  [32704/60000]\n",
            "loss: 572.328735  [33344/60000]\n",
            "loss: 1856.603394  [33984/60000]\n",
            "loss: 454.445770  [34624/60000]\n",
            "loss: 818.888428  [35264/60000]\n",
            "loss: 1072.982422  [35904/60000]\n",
            "loss: 1227.258057  [36544/60000]\n",
            "loss: 3041.121094  [37184/60000]\n",
            "loss: 1086.791138  [37824/60000]\n",
            "loss: 1221.568115  [38464/60000]\n",
            "loss: 1499.767700  [39104/60000]\n",
            "loss: 1379.731934  [39744/60000]\n",
            "loss: 828.249451  [40384/60000]\n",
            "loss: 1494.039429  [41024/60000]\n",
            "loss: 1215.152588  [41664/60000]\n",
            "loss: 1007.311707  [42304/60000]\n",
            "loss: 2263.409668  [42944/60000]\n",
            "loss: 670.046997  [43584/60000]\n",
            "loss: 2242.288330  [44224/60000]\n",
            "loss: 1871.966797  [44864/60000]\n",
            "loss: 688.122498  [45504/60000]\n",
            "loss: 1147.889404  [46144/60000]\n",
            "loss: 1166.516113  [46784/60000]\n",
            "loss: 500.503662  [47424/60000]\n",
            "loss: 839.729614  [48064/60000]\n",
            "loss: 2129.783447  [48704/60000]\n",
            "loss: 1127.403809  [49344/60000]\n",
            "loss: 1476.402466  [49984/60000]\n",
            "loss: 1168.983521  [50624/60000]\n",
            "loss: 2546.857666  [51264/60000]\n",
            "loss: 3871.785156  [51904/60000]\n",
            "loss: 4024.356445  [52544/60000]\n",
            "loss: 2758.353516  [53184/60000]\n",
            "loss: 859.905151  [53824/60000]\n",
            "loss: 1133.844238  [54464/60000]\n",
            "loss: 974.434631  [55104/60000]\n",
            "loss: 887.479004  [55744/60000]\n",
            "loss: 870.423645  [56384/60000]\n",
            "loss: 1034.829590  [57024/60000]\n",
            "loss: 1345.233398  [57664/60000]\n",
            "loss: 643.275452  [58304/60000]\n",
            "loss: 924.882935  [58944/60000]\n",
            "loss: 1749.554077  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 2346.405250 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1808.404541  [   64/60000]\n",
            "loss: 1025.010132  [  704/60000]\n",
            "loss: 550.759277  [ 1344/60000]\n",
            "loss: 824.279846  [ 1984/60000]\n",
            "loss: 1376.454590  [ 2624/60000]\n",
            "loss: 1031.027466  [ 3264/60000]\n",
            "loss: 2465.273438  [ 3904/60000]\n",
            "loss: 910.672363  [ 4544/60000]\n",
            "loss: 1217.129395  [ 5184/60000]\n",
            "loss: 1122.464844  [ 5824/60000]\n",
            "loss: 566.772217  [ 6464/60000]\n",
            "loss: 2695.429932  [ 7104/60000]\n",
            "loss: 2561.869141  [ 7744/60000]\n",
            "loss: 1530.843872  [ 8384/60000]\n",
            "loss: 1086.732422  [ 9024/60000]\n",
            "loss: 729.898315  [ 9664/60000]\n",
            "loss: 652.915588  [10304/60000]\n",
            "loss: 1197.466309  [10944/60000]\n",
            "loss: 920.794250  [11584/60000]\n",
            "loss: 2726.102051  [12224/60000]\n",
            "loss: 529.147217  [12864/60000]\n",
            "loss: 692.957336  [13504/60000]\n",
            "loss: 672.539490  [14144/60000]\n",
            "loss: 1000.459839  [14784/60000]\n",
            "loss: 868.087891  [15424/60000]\n",
            "loss: 1491.407227  [16064/60000]\n",
            "loss: 1882.125854  [16704/60000]\n",
            "loss: 1552.711914  [17344/60000]\n",
            "loss: 1810.646240  [17984/60000]\n",
            "loss: 781.620850  [18624/60000]\n",
            "loss: 1212.600952  [19264/60000]\n",
            "loss: 1352.452026  [19904/60000]\n",
            "loss: 1284.001953  [20544/60000]\n",
            "loss: 703.841553  [21184/60000]\n",
            "loss: 3488.074707  [21824/60000]\n",
            "loss: 2429.854736  [22464/60000]\n",
            "loss: 2253.203613  [23104/60000]\n",
            "loss: 940.689148  [23744/60000]\n",
            "loss: 939.598083  [24384/60000]\n",
            "loss: 718.560303  [25024/60000]\n",
            "loss: 878.048157  [25664/60000]\n",
            "loss: 1349.545776  [26304/60000]\n",
            "loss: 1364.406738  [26944/60000]\n",
            "loss: 1890.635132  [27584/60000]\n",
            "loss: 896.372192  [28224/60000]\n",
            "loss: 2215.606445  [28864/60000]\n",
            "loss: 1763.191895  [29504/60000]\n",
            "loss: 2426.615723  [30144/60000]\n",
            "loss: 1328.180542  [30784/60000]\n",
            "loss: 593.210510  [31424/60000]\n",
            "loss: 744.770569  [32064/60000]\n",
            "loss: 1480.967896  [32704/60000]\n",
            "loss: 636.766296  [33344/60000]\n",
            "loss: 1794.912109  [33984/60000]\n",
            "loss: 594.138855  [34624/60000]\n",
            "loss: 1136.428955  [35264/60000]\n",
            "loss: 818.077637  [35904/60000]\n",
            "loss: 2182.771240  [36544/60000]\n",
            "loss: 2867.920898  [37184/60000]\n",
            "loss: 1053.543701  [37824/60000]\n",
            "loss: 1056.515259  [38464/60000]\n",
            "loss: 1513.221191  [39104/60000]\n",
            "loss: 1851.013794  [39744/60000]\n",
            "loss: 1409.898926  [40384/60000]\n",
            "loss: 2042.999512  [41024/60000]\n",
            "loss: 1073.974243  [41664/60000]\n",
            "loss: 1100.510986  [42304/60000]\n",
            "loss: 3392.239746  [42944/60000]\n",
            "loss: 539.881348  [43584/60000]\n",
            "loss: 557.422607  [44224/60000]\n",
            "loss: 1310.921997  [44864/60000]\n",
            "loss: 496.403931  [45504/60000]\n",
            "loss: 731.343628  [46144/60000]\n",
            "loss: 1113.668457  [46784/60000]\n",
            "loss: 954.488342  [47424/60000]\n",
            "loss: 1251.460815  [48064/60000]\n",
            "loss: 864.442688  [48704/60000]\n",
            "loss: 901.740051  [49344/60000]\n",
            "loss: 1409.234131  [49984/60000]\n",
            "loss: 1440.451416  [50624/60000]\n",
            "loss: 2308.387207  [51264/60000]\n",
            "loss: 4216.517578  [51904/60000]\n",
            "loss: 2420.836426  [52544/60000]\n",
            "loss: 1632.854004  [53184/60000]\n",
            "loss: 1022.358032  [53824/60000]\n",
            "loss: 836.196045  [54464/60000]\n",
            "loss: 1071.924072  [55104/60000]\n",
            "loss: 675.437012  [55744/60000]\n",
            "loss: 2312.957520  [56384/60000]\n",
            "loss: 1194.183838  [57024/60000]\n",
            "loss: 947.500793  [57664/60000]\n",
            "loss: 894.606506  [58304/60000]\n",
            "loss: 954.675415  [58944/60000]\n",
            "loss: 2097.445312  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.5%, Avg loss: 1170.316776 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 725.152222  [   64/60000]\n",
            "loss: 846.936340  [  704/60000]\n",
            "loss: 450.223785  [ 1344/60000]\n",
            "loss: 689.016479  [ 1984/60000]\n",
            "loss: 1102.831055  [ 2624/60000]\n",
            "loss: 951.509705  [ 3264/60000]\n",
            "loss: 1376.993042  [ 3904/60000]\n",
            "loss: 787.865479  [ 4544/60000]\n",
            "loss: 3295.919434  [ 5184/60000]\n",
            "loss: 1929.779175  [ 5824/60000]\n",
            "loss: 618.960449  [ 6464/60000]\n",
            "loss: 1254.264282  [ 7104/60000]\n",
            "loss: 1595.746094  [ 7744/60000]\n",
            "loss: 1818.946655  [ 8384/60000]\n",
            "loss: 806.739014  [ 9024/60000]\n",
            "loss: 936.297485  [ 9664/60000]\n",
            "loss: 788.452332  [10304/60000]\n",
            "loss: 1034.395508  [10944/60000]\n",
            "loss: 1092.940796  [11584/60000]\n",
            "loss: 2820.160156  [12224/60000]\n",
            "loss: 2861.884766  [12864/60000]\n",
            "loss: 974.792725  [13504/60000]\n",
            "loss: 2055.117188  [14144/60000]\n",
            "loss: 1046.034302  [14784/60000]\n",
            "loss: 686.988525  [15424/60000]\n",
            "loss: 815.880920  [16064/60000]\n",
            "loss: 811.099487  [16704/60000]\n",
            "loss: 1878.266602  [17344/60000]\n",
            "loss: 1038.245483  [17984/60000]\n",
            "loss: 650.201904  [18624/60000]\n",
            "loss: 1775.330566  [19264/60000]\n",
            "loss: 2314.612793  [19904/60000]\n",
            "loss: 558.498352  [20544/60000]\n",
            "loss: 711.318726  [21184/60000]\n",
            "loss: 3921.746826  [21824/60000]\n",
            "loss: 1849.130859  [22464/60000]\n",
            "loss: 1105.464111  [23104/60000]\n",
            "loss: 901.291748  [23744/60000]\n",
            "loss: 2963.632812  [24384/60000]\n",
            "loss: 797.879761  [25024/60000]\n",
            "loss: 1269.734253  [25664/60000]\n",
            "loss: 2506.648193  [26304/60000]\n",
            "loss: 1317.180176  [26944/60000]\n",
            "loss: 1991.641846  [27584/60000]\n",
            "loss: 602.514160  [28224/60000]\n",
            "loss: 803.927979  [28864/60000]\n",
            "loss: 1602.685547  [29504/60000]\n",
            "loss: 1352.807861  [30144/60000]\n",
            "loss: 1114.225098  [30784/60000]\n",
            "loss: 510.117981  [31424/60000]\n",
            "loss: 1202.289795  [32064/60000]\n",
            "loss: 744.492126  [32704/60000]\n",
            "loss: 419.897705  [33344/60000]\n",
            "loss: 2269.041504  [33984/60000]\n",
            "loss: 786.588196  [34624/60000]\n",
            "loss: 538.799377  [35264/60000]\n",
            "loss: 1156.328125  [35904/60000]\n",
            "loss: 1075.342163  [36544/60000]\n",
            "loss: 1475.264648  [37184/60000]\n",
            "loss: 430.430573  [37824/60000]\n",
            "loss: 1102.487793  [38464/60000]\n",
            "loss: 633.539795  [39104/60000]\n",
            "loss: 1459.587402  [39744/60000]\n",
            "loss: 1530.669434  [40384/60000]\n",
            "loss: 1802.510620  [41024/60000]\n",
            "loss: 848.734741  [41664/60000]\n",
            "loss: 1974.730225  [42304/60000]\n",
            "loss: 1132.574463  [42944/60000]\n",
            "loss: 855.754272  [43584/60000]\n",
            "loss: 1376.994873  [44224/60000]\n",
            "loss: 933.893250  [44864/60000]\n",
            "loss: 679.489075  [45504/60000]\n",
            "loss: 1056.309448  [46144/60000]\n",
            "loss: 711.928711  [46784/60000]\n",
            "loss: 2859.372559  [47424/60000]\n",
            "loss: 1422.294922  [48064/60000]\n",
            "loss: 1194.407837  [48704/60000]\n",
            "loss: 816.130859  [49344/60000]\n",
            "loss: 1086.079102  [49984/60000]\n",
            "loss: 1159.410400  [50624/60000]\n",
            "loss: 2572.079346  [51264/60000]\n",
            "loss: 4231.126465  [51904/60000]\n",
            "loss: 3935.714600  [52544/60000]\n",
            "loss: 2628.560791  [53184/60000]\n",
            "loss: 1176.711182  [53824/60000]\n",
            "loss: 1663.035767  [54464/60000]\n",
            "loss: 1025.185791  [55104/60000]\n",
            "loss: 1752.478882  [55744/60000]\n",
            "loss: 1531.232788  [56384/60000]\n",
            "loss: 1160.478271  [57024/60000]\n",
            "loss: 938.366516  [57664/60000]\n",
            "loss: 1005.557617  [58304/60000]\n",
            "loss: 816.720093  [58944/60000]\n",
            "loss: 2254.406006  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.5%, Avg loss: 2533.619784 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1961.642944  [   64/60000]\n",
            "loss: 1157.138184  [  704/60000]\n",
            "loss: 1208.042114  [ 1344/60000]\n",
            "loss: 1145.676758  [ 1984/60000]\n",
            "loss: 1068.421387  [ 2624/60000]\n",
            "loss: 857.487793  [ 3264/60000]\n",
            "loss: 1188.783447  [ 3904/60000]\n",
            "loss: 910.757141  [ 4544/60000]\n",
            "loss: 1555.609375  [ 5184/60000]\n",
            "loss: 779.527588  [ 5824/60000]\n",
            "loss: 602.275879  [ 6464/60000]\n",
            "loss: 947.291382  [ 7104/60000]\n",
            "loss: 2190.252441  [ 7744/60000]\n",
            "loss: 1543.929688  [ 8384/60000]\n",
            "loss: 1412.039307  [ 9024/60000]\n",
            "loss: 872.004700  [ 9664/60000]\n",
            "loss: 1388.349365  [10304/60000]\n",
            "loss: 1844.632812  [10944/60000]\n",
            "loss: 1570.451904  [11584/60000]\n",
            "loss: 1494.381714  [12224/60000]\n",
            "loss: 997.084229  [12864/60000]\n",
            "loss: 1013.506226  [13504/60000]\n",
            "loss: 426.721863  [14144/60000]\n",
            "loss: 783.353027  [14784/60000]\n",
            "loss: 725.003479  [15424/60000]\n",
            "loss: 866.862671  [16064/60000]\n",
            "loss: 886.810669  [16704/60000]\n",
            "loss: 1408.758057  [17344/60000]\n",
            "loss: 1095.588257  [17984/60000]\n",
            "loss: 639.631226  [18624/60000]\n",
            "loss: 892.970154  [19264/60000]\n",
            "loss: 1820.189209  [19904/60000]\n",
            "loss: 614.915894  [20544/60000]\n",
            "loss: 690.533691  [21184/60000]\n",
            "loss: 2710.419434  [21824/60000]\n",
            "loss: 1631.987793  [22464/60000]\n",
            "loss: 2493.080078  [23104/60000]\n",
            "loss: 808.135742  [23744/60000]\n",
            "loss: 1043.219482  [24384/60000]\n",
            "loss: 812.639771  [25024/60000]\n",
            "loss: 2333.716797  [25664/60000]\n",
            "loss: 3034.596436  [26304/60000]\n",
            "loss: 1074.005859  [26944/60000]\n",
            "loss: 1736.871460  [27584/60000]\n",
            "loss: 534.779663  [28224/60000]\n",
            "loss: 613.951355  [28864/60000]\n",
            "loss: 1705.482300  [29504/60000]\n",
            "loss: 1513.023804  [30144/60000]\n",
            "loss: 795.113892  [30784/60000]\n",
            "loss: 539.437256  [31424/60000]\n",
            "loss: 1589.497803  [32064/60000]\n",
            "loss: 907.807556  [32704/60000]\n",
            "loss: 618.072144  [33344/60000]\n",
            "loss: 2560.857910  [33984/60000]\n",
            "loss: 543.374390  [34624/60000]\n",
            "loss: 480.340179  [35264/60000]\n",
            "loss: 627.889893  [35904/60000]\n",
            "loss: 949.152161  [36544/60000]\n",
            "loss: 3063.883789  [37184/60000]\n",
            "loss: 435.227997  [37824/60000]\n",
            "loss: 829.391907  [38464/60000]\n",
            "loss: 760.205688  [39104/60000]\n",
            "loss: 1594.395508  [39744/60000]\n",
            "loss: 892.418518  [40384/60000]\n",
            "loss: 1517.400757  [41024/60000]\n",
            "loss: 3493.427002  [41664/60000]\n",
            "loss: 1573.871948  [42304/60000]\n",
            "loss: 1301.890381  [42944/60000]\n",
            "loss: 1605.403809  [43584/60000]\n",
            "loss: 1014.108154  [44224/60000]\n",
            "loss: 1261.336548  [44864/60000]\n",
            "loss: 561.002808  [45504/60000]\n",
            "loss: 1607.793945  [46144/60000]\n",
            "loss: 1021.278381  [46784/60000]\n",
            "loss: 500.798370  [47424/60000]\n",
            "loss: 1033.425903  [48064/60000]\n",
            "loss: 1238.124023  [48704/60000]\n",
            "loss: 1050.518311  [49344/60000]\n",
            "loss: 1163.354980  [49984/60000]\n",
            "loss: 608.193970  [50624/60000]\n",
            "loss: 1249.222534  [51264/60000]\n",
            "loss: 2846.935547  [51904/60000]\n",
            "loss: 2888.745117  [52544/60000]\n",
            "loss: 562.427856  [53184/60000]\n",
            "loss: 794.908142  [53824/60000]\n",
            "loss: 1368.368164  [54464/60000]\n",
            "loss: 486.782623  [55104/60000]\n",
            "loss: 489.752472  [55744/60000]\n",
            "loss: 707.369446  [56384/60000]\n",
            "loss: 1007.224121  [57024/60000]\n",
            "loss: 569.141663  [57664/60000]\n",
            "loss: 426.521240  [58304/60000]\n",
            "loss: 702.360962  [58944/60000]\n",
            "loss: 2040.648926  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 1993.530759 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1353.208740  [   64/60000]\n",
            "loss: 868.184204  [  704/60000]\n",
            "loss: 573.065918  [ 1344/60000]\n",
            "loss: 948.787476  [ 1984/60000]\n",
            "loss: 995.311035  [ 2624/60000]\n",
            "loss: 1664.308350  [ 3264/60000]\n",
            "loss: 1219.635742  [ 3904/60000]\n",
            "loss: 812.884521  [ 4544/60000]\n",
            "loss: 3009.521240  [ 5184/60000]\n",
            "loss: 518.206604  [ 5824/60000]\n",
            "loss: 673.936768  [ 6464/60000]\n",
            "loss: 809.441162  [ 7104/60000]\n",
            "loss: 1419.504395  [ 7744/60000]\n",
            "loss: 998.293457  [ 8384/60000]\n",
            "loss: 1039.929321  [ 9024/60000]\n",
            "loss: 1657.535889  [ 9664/60000]\n",
            "loss: 1000.514160  [10304/60000]\n",
            "loss: 586.193787  [10944/60000]\n",
            "loss: 893.329224  [11584/60000]\n",
            "loss: 2681.473145  [12224/60000]\n",
            "loss: 3098.759766  [12864/60000]\n",
            "loss: 2074.134521  [13504/60000]\n",
            "loss: 2542.533691  [14144/60000]\n",
            "loss: 2644.087891  [14784/60000]\n",
            "loss: 521.628540  [15424/60000]\n",
            "loss: 1099.093018  [16064/60000]\n",
            "loss: 1397.048340  [16704/60000]\n",
            "loss: 1365.998291  [17344/60000]\n",
            "loss: 950.909180  [17984/60000]\n",
            "loss: 637.586121  [18624/60000]\n",
            "loss: 1318.574585  [19264/60000]\n",
            "loss: 1305.261230  [19904/60000]\n",
            "loss: 597.345032  [20544/60000]\n",
            "loss: 873.293945  [21184/60000]\n",
            "loss: 2997.710449  [21824/60000]\n",
            "loss: 999.073120  [22464/60000]\n",
            "loss: 2283.827148  [23104/60000]\n",
            "loss: 922.522095  [23744/60000]\n",
            "loss: 1067.692993  [24384/60000]\n",
            "loss: 757.381958  [25024/60000]\n",
            "loss: 1027.021851  [25664/60000]\n",
            "loss: 2507.149414  [26304/60000]\n",
            "loss: 1067.598999  [26944/60000]\n",
            "loss: 1705.299683  [27584/60000]\n",
            "loss: 705.359192  [28224/60000]\n",
            "loss: 398.568726  [28864/60000]\n",
            "loss: 1679.895264  [29504/60000]\n",
            "loss: 1724.303223  [30144/60000]\n",
            "loss: 1119.658691  [30784/60000]\n",
            "loss: 515.650024  [31424/60000]\n",
            "loss: 999.292358  [32064/60000]\n",
            "loss: 624.046021  [32704/60000]\n",
            "loss: 731.176025  [33344/60000]\n",
            "loss: 1644.554688  [33984/60000]\n",
            "loss: 599.811218  [34624/60000]\n",
            "loss: 751.663452  [35264/60000]\n",
            "loss: 583.939697  [35904/60000]\n",
            "loss: 1120.484253  [36544/60000]\n",
            "loss: 2846.484375  [37184/60000]\n",
            "loss: 390.430176  [37824/60000]\n",
            "loss: 1245.572876  [38464/60000]\n",
            "loss: 1677.712158  [39104/60000]\n",
            "loss: 1548.668457  [39744/60000]\n",
            "loss: 953.304993  [40384/60000]\n",
            "loss: 1820.091675  [41024/60000]\n",
            "loss: 973.991272  [41664/60000]\n",
            "loss: 1866.658936  [42304/60000]\n",
            "loss: 3618.104980  [42944/60000]\n",
            "loss: 1602.301025  [43584/60000]\n",
            "loss: 1960.305908  [44224/60000]\n",
            "loss: 986.408447  [44864/60000]\n",
            "loss: 824.119873  [45504/60000]\n",
            "loss: 995.176025  [46144/60000]\n",
            "loss: 997.522949  [46784/60000]\n",
            "loss: 1342.937256  [47424/60000]\n",
            "loss: 1345.842529  [48064/60000]\n",
            "loss: 801.806458  [48704/60000]\n",
            "loss: 913.356934  [49344/60000]\n",
            "loss: 1624.762695  [49984/60000]\n",
            "loss: 542.587280  [50624/60000]\n",
            "loss: 1558.831787  [51264/60000]\n",
            "loss: 2089.170654  [51904/60000]\n",
            "loss: 4509.346680  [52544/60000]\n",
            "loss: 1428.356689  [53184/60000]\n",
            "loss: 762.908875  [53824/60000]\n",
            "loss: 1865.532471  [54464/60000]\n",
            "loss: 760.693176  [55104/60000]\n",
            "loss: 694.192017  [55744/60000]\n",
            "loss: 655.164062  [56384/60000]\n",
            "loss: 1165.429810  [57024/60000]\n",
            "loss: 1378.059204  [57664/60000]\n",
            "loss: 836.398071  [58304/60000]\n",
            "loss: 624.647461  [58944/60000]\n",
            "loss: 2806.891357  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 3079.955758 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2321.578125  [   64/60000]\n",
            "loss: 1610.536011  [  704/60000]\n",
            "loss: 377.516418  [ 1344/60000]\n",
            "loss: 1312.286133  [ 1984/60000]\n",
            "loss: 1010.384155  [ 2624/60000]\n",
            "loss: 952.099915  [ 3264/60000]\n",
            "loss: 1389.266113  [ 3904/60000]\n",
            "loss: 1081.633545  [ 4544/60000]\n",
            "loss: 2690.526611  [ 5184/60000]\n",
            "loss: 1543.645020  [ 5824/60000]\n",
            "loss: 663.156250  [ 6464/60000]\n",
            "loss: 829.101379  [ 7104/60000]\n",
            "loss: 1609.574219  [ 7744/60000]\n",
            "loss: 1077.621460  [ 8384/60000]\n",
            "loss: 1071.455444  [ 9024/60000]\n",
            "loss: 1283.723267  [ 9664/60000]\n",
            "loss: 2118.161377  [10304/60000]\n",
            "loss: 788.605530  [10944/60000]\n",
            "loss: 890.696045  [11584/60000]\n",
            "loss: 4134.012207  [12224/60000]\n",
            "loss: 1114.213867  [12864/60000]\n",
            "loss: 1569.147583  [13504/60000]\n",
            "loss: 391.389587  [14144/60000]\n",
            "loss: 3026.577148  [14784/60000]\n",
            "loss: 237.186554  [15424/60000]\n",
            "loss: 1116.237061  [16064/60000]\n",
            "loss: 1194.849609  [16704/60000]\n",
            "loss: 1282.731323  [17344/60000]\n",
            "loss: 858.415405  [17984/60000]\n",
            "loss: 704.960693  [18624/60000]\n",
            "loss: 1615.471924  [19264/60000]\n",
            "loss: 1652.425537  [19904/60000]\n",
            "loss: 877.410278  [20544/60000]\n",
            "loss: 879.466675  [21184/60000]\n",
            "loss: 4167.981934  [21824/60000]\n",
            "loss: 1431.347778  [22464/60000]\n",
            "loss: 747.727722  [23104/60000]\n",
            "loss: 642.433289  [23744/60000]\n",
            "loss: 2173.699707  [24384/60000]\n",
            "loss: 735.957275  [25024/60000]\n",
            "loss: 851.407715  [25664/60000]\n",
            "loss: 2044.139771  [26304/60000]\n",
            "loss: 955.936768  [26944/60000]\n",
            "loss: 2163.547852  [27584/60000]\n",
            "loss: 618.842651  [28224/60000]\n",
            "loss: 1524.292725  [28864/60000]\n",
            "loss: 1764.891479  [29504/60000]\n",
            "loss: 1492.492798  [30144/60000]\n",
            "loss: 1144.585449  [30784/60000]\n",
            "loss: 343.720276  [31424/60000]\n",
            "loss: 1156.663208  [32064/60000]\n",
            "loss: 863.616638  [32704/60000]\n",
            "loss: 653.953613  [33344/60000]\n",
            "loss: 1293.068848  [33984/60000]\n",
            "loss: 538.982422  [34624/60000]\n",
            "loss: 464.445648  [35264/60000]\n",
            "loss: 573.819946  [35904/60000]\n",
            "loss: 887.106079  [36544/60000]\n",
            "loss: 2415.136475  [37184/60000]\n",
            "loss: 831.323669  [37824/60000]\n",
            "loss: 1262.878906  [38464/60000]\n",
            "loss: 1430.781494  [39104/60000]\n",
            "loss: 1389.711426  [39744/60000]\n",
            "loss: 874.233826  [40384/60000]\n",
            "loss: 1492.101318  [41024/60000]\n",
            "loss: 1011.681396  [41664/60000]\n",
            "loss: 1320.494263  [42304/60000]\n",
            "loss: 3315.118652  [42944/60000]\n",
            "loss: 1856.119629  [43584/60000]\n",
            "loss: 2041.311157  [44224/60000]\n",
            "loss: 1172.108276  [44864/60000]\n",
            "loss: 950.938538  [45504/60000]\n",
            "loss: 850.440552  [46144/60000]\n",
            "loss: 1613.152710  [46784/60000]\n",
            "loss: 503.207397  [47424/60000]\n",
            "loss: 1025.716797  [48064/60000]\n",
            "loss: 1221.260620  [48704/60000]\n",
            "loss: 969.909424  [49344/60000]\n",
            "loss: 1784.250366  [49984/60000]\n",
            "loss: 500.068085  [50624/60000]\n",
            "loss: 1767.551636  [51264/60000]\n",
            "loss: 3752.777832  [51904/60000]\n",
            "loss: 3612.713135  [52544/60000]\n",
            "loss: 1202.818237  [53184/60000]\n",
            "loss: 820.001770  [53824/60000]\n",
            "loss: 1192.563965  [54464/60000]\n",
            "loss: 608.484253  [55104/60000]\n",
            "loss: 664.022156  [55744/60000]\n",
            "loss: 1222.148438  [56384/60000]\n",
            "loss: 1351.836670  [57024/60000]\n",
            "loss: 723.367188  [57664/60000]\n",
            "loss: 612.124817  [58304/60000]\n",
            "loss: 928.726624  [58944/60000]\n",
            "loss: 1794.700073  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.4%, Avg loss: 1501.137007 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 871.005859  [   64/60000]\n",
            "loss: 936.781372  [  704/60000]\n",
            "loss: 1804.963013  [ 1344/60000]\n",
            "loss: 727.262268  [ 1984/60000]\n",
            "loss: 1172.766357  [ 2624/60000]\n",
            "loss: 981.850586  [ 3264/60000]\n",
            "loss: 1124.098755  [ 3904/60000]\n",
            "loss: 680.624634  [ 4544/60000]\n",
            "loss: 1784.245728  [ 5184/60000]\n",
            "loss: 574.624451  [ 5824/60000]\n",
            "loss: 429.710022  [ 6464/60000]\n",
            "loss: 1715.592529  [ 7104/60000]\n",
            "loss: 1769.687134  [ 7744/60000]\n",
            "loss: 1689.151978  [ 8384/60000]\n",
            "loss: 1094.394775  [ 9024/60000]\n",
            "loss: 1310.992310  [ 9664/60000]\n",
            "loss: 752.234131  [10304/60000]\n",
            "loss: 554.975830  [10944/60000]\n",
            "loss: 1170.849487  [11584/60000]\n",
            "loss: 1749.043701  [12224/60000]\n",
            "loss: 1081.139893  [12864/60000]\n",
            "loss: 1923.670898  [13504/60000]\n",
            "loss: 1925.907349  [14144/60000]\n",
            "loss: 1130.803833  [14784/60000]\n",
            "loss: 448.876648  [15424/60000]\n",
            "loss: 2093.528076  [16064/60000]\n",
            "loss: 1431.351929  [16704/60000]\n",
            "loss: 1417.198242  [17344/60000]\n",
            "loss: 943.364502  [17984/60000]\n",
            "loss: 611.453491  [18624/60000]\n",
            "loss: 1262.360718  [19264/60000]\n",
            "loss: 1643.249146  [19904/60000]\n",
            "loss: 630.649475  [20544/60000]\n",
            "loss: 746.392639  [21184/60000]\n",
            "loss: 2949.260010  [21824/60000]\n",
            "loss: 628.383972  [22464/60000]\n",
            "loss: 991.877319  [23104/60000]\n",
            "loss: 892.434692  [23744/60000]\n",
            "loss: 1865.910034  [24384/60000]\n",
            "loss: 739.502197  [25024/60000]\n",
            "loss: 784.842896  [25664/60000]\n",
            "loss: 2112.422852  [26304/60000]\n",
            "loss: 1276.230103  [26944/60000]\n",
            "loss: 1413.703979  [27584/60000]\n",
            "loss: 724.661438  [28224/60000]\n",
            "loss: 315.904663  [28864/60000]\n",
            "loss: 1313.231934  [29504/60000]\n",
            "loss: 2274.845459  [30144/60000]\n",
            "loss: 794.933899  [30784/60000]\n",
            "loss: 690.674561  [31424/60000]\n",
            "loss: 1141.471191  [32064/60000]\n",
            "loss: 827.147583  [32704/60000]\n",
            "loss: 475.896729  [33344/60000]\n",
            "loss: 1421.490967  [33984/60000]\n",
            "loss: 438.352173  [34624/60000]\n",
            "loss: 954.819824  [35264/60000]\n",
            "loss: 1154.907715  [35904/60000]\n",
            "loss: 726.708069  [36544/60000]\n",
            "loss: 2559.054199  [37184/60000]\n",
            "loss: 480.276306  [37824/60000]\n",
            "loss: 872.377258  [38464/60000]\n",
            "loss: 570.045471  [39104/60000]\n",
            "loss: 1200.594116  [39744/60000]\n",
            "loss: 1129.065186  [40384/60000]\n",
            "loss: 1776.994873  [41024/60000]\n",
            "loss: 948.049500  [41664/60000]\n",
            "loss: 960.436768  [42304/60000]\n",
            "loss: 1457.933594  [42944/60000]\n",
            "loss: 2354.021484  [43584/60000]\n",
            "loss: 2381.333252  [44224/60000]\n",
            "loss: 935.153931  [44864/60000]\n",
            "loss: 740.223145  [45504/60000]\n",
            "loss: 815.014526  [46144/60000]\n",
            "loss: 1626.842896  [46784/60000]\n",
            "loss: 1270.253296  [47424/60000]\n",
            "loss: 1124.905640  [48064/60000]\n",
            "loss: 1180.963013  [48704/60000]\n",
            "loss: 854.873352  [49344/60000]\n",
            "loss: 1327.252686  [49984/60000]\n",
            "loss: 649.779724  [50624/60000]\n",
            "loss: 1664.138062  [51264/60000]\n",
            "loss: 2403.928711  [51904/60000]\n",
            "loss: 2936.642822  [52544/60000]\n",
            "loss: 1115.620605  [53184/60000]\n",
            "loss: 1289.097656  [53824/60000]\n",
            "loss: 866.953491  [54464/60000]\n",
            "loss: 1091.469238  [55104/60000]\n",
            "loss: 1120.692139  [55744/60000]\n",
            "loss: 815.699829  [56384/60000]\n",
            "loss: 1309.354492  [57024/60000]\n",
            "loss: 812.866150  [57664/60000]\n",
            "loss: 698.470581  [58304/60000]\n",
            "loss: 660.476868  [58944/60000]\n",
            "loss: 1900.791504  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 1730.774506 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1020.157593  [   64/60000]\n",
            "loss: 587.348267  [  704/60000]\n",
            "loss: 999.154541  [ 1344/60000]\n",
            "loss: 1423.692993  [ 1984/60000]\n",
            "loss: 1230.923584  [ 2624/60000]\n",
            "loss: 913.922546  [ 3264/60000]\n",
            "loss: 1468.414795  [ 3904/60000]\n",
            "loss: 667.856812  [ 4544/60000]\n",
            "loss: 1178.704834  [ 5184/60000]\n",
            "loss: 1171.079224  [ 5824/60000]\n",
            "loss: 472.741943  [ 6464/60000]\n",
            "loss: 1433.479248  [ 7104/60000]\n",
            "loss: 1957.986206  [ 7744/60000]\n",
            "loss: 1225.675537  [ 8384/60000]\n",
            "loss: 948.682495  [ 9024/60000]\n",
            "loss: 1015.978638  [ 9664/60000]\n",
            "loss: 803.842163  [10304/60000]\n",
            "loss: 554.773071  [10944/60000]\n",
            "loss: 1033.860107  [11584/60000]\n",
            "loss: 2398.010498  [12224/60000]\n",
            "loss: 902.670288  [12864/60000]\n",
            "loss: 1063.837891  [13504/60000]\n",
            "loss: 385.765442  [14144/60000]\n",
            "loss: 801.667358  [14784/60000]\n",
            "loss: 265.169617  [15424/60000]\n",
            "loss: 1940.741577  [16064/60000]\n",
            "loss: 1239.744019  [16704/60000]\n",
            "loss: 1320.148682  [17344/60000]\n",
            "loss: 1508.837646  [17984/60000]\n",
            "loss: 840.323669  [18624/60000]\n",
            "loss: 1253.443115  [19264/60000]\n",
            "loss: 1836.630859  [19904/60000]\n",
            "loss: 428.629395  [20544/60000]\n",
            "loss: 773.619019  [21184/60000]\n",
            "loss: 3496.142090  [21824/60000]\n",
            "loss: 859.967163  [22464/60000]\n",
            "loss: 1345.941162  [23104/60000]\n",
            "loss: 832.887451  [23744/60000]\n",
            "loss: 1749.128174  [24384/60000]\n",
            "loss: 852.536743  [25024/60000]\n",
            "loss: 1458.244141  [25664/60000]\n",
            "loss: 2089.369629  [26304/60000]\n",
            "loss: 1002.961670  [26944/60000]\n",
            "loss: 1476.222412  [27584/60000]\n",
            "loss: 657.834717  [28224/60000]\n",
            "loss: 394.422516  [28864/60000]\n",
            "loss: 1033.884644  [29504/60000]\n",
            "loss: 2729.715332  [30144/60000]\n",
            "loss: 893.247437  [30784/60000]\n",
            "loss: 317.097900  [31424/60000]\n",
            "loss: 1154.105835  [32064/60000]\n",
            "loss: 762.699707  [32704/60000]\n",
            "loss: 520.550659  [33344/60000]\n",
            "loss: 1659.667358  [33984/60000]\n",
            "loss: 434.156647  [34624/60000]\n",
            "loss: 490.002258  [35264/60000]\n",
            "loss: 694.876343  [35904/60000]\n",
            "loss: 750.472412  [36544/60000]\n",
            "loss: 1860.009033  [37184/60000]\n",
            "loss: 440.956757  [37824/60000]\n",
            "loss: 1629.424438  [38464/60000]\n",
            "loss: 1572.366089  [39104/60000]\n",
            "loss: 1609.951660  [39744/60000]\n",
            "loss: 1683.522217  [40384/60000]\n",
            "loss: 2095.741211  [41024/60000]\n",
            "loss: 1132.973633  [41664/60000]\n",
            "loss: 1595.527100  [42304/60000]\n",
            "loss: 559.522095  [42944/60000]\n",
            "loss: 1427.403564  [43584/60000]\n",
            "loss: 2012.382690  [44224/60000]\n",
            "loss: 1424.008911  [44864/60000]\n",
            "loss: 1132.891357  [45504/60000]\n",
            "loss: 779.346375  [46144/60000]\n",
            "loss: 1624.312500  [46784/60000]\n",
            "loss: 579.019836  [47424/60000]\n",
            "loss: 1200.509277  [48064/60000]\n",
            "loss: 795.408997  [48704/60000]\n",
            "loss: 681.061768  [49344/60000]\n",
            "loss: 1512.751465  [49984/60000]\n",
            "loss: 832.451355  [50624/60000]\n",
            "loss: 2405.541992  [51264/60000]\n",
            "loss: 3471.771484  [51904/60000]\n",
            "loss: 3049.365723  [52544/60000]\n",
            "loss: 1546.234619  [53184/60000]\n",
            "loss: 1120.085815  [53824/60000]\n",
            "loss: 2293.152588  [54464/60000]\n",
            "loss: 599.906006  [55104/60000]\n",
            "loss: 882.621033  [55744/60000]\n",
            "loss: 679.386597  [56384/60000]\n",
            "loss: 988.243408  [57024/60000]\n",
            "loss: 1076.595337  [57664/60000]\n",
            "loss: 259.829102  [58304/60000]\n",
            "loss: 705.734741  [58944/60000]\n",
            "loss: 2082.158936  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.1%, Avg loss: 2679.333700 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1983.590332  [   64/60000]\n",
            "loss: 1235.825195  [  704/60000]\n",
            "loss: 412.768188  [ 1344/60000]\n",
            "loss: 890.292847  [ 1984/60000]\n",
            "loss: 1051.250000  [ 2624/60000]\n",
            "loss: 986.728516  [ 3264/60000]\n",
            "loss: 1348.010498  [ 3904/60000]\n",
            "loss: 455.758545  [ 4544/60000]\n",
            "loss: 1946.741699  [ 5184/60000]\n",
            "loss: 653.969971  [ 5824/60000]\n",
            "loss: 316.747650  [ 6464/60000]\n",
            "loss: 611.160034  [ 7104/60000]\n",
            "loss: 2270.442383  [ 7744/60000]\n",
            "loss: 1247.340088  [ 8384/60000]\n",
            "loss: 916.152649  [ 9024/60000]\n",
            "loss: 2388.006836  [ 9664/60000]\n",
            "loss: 887.702087  [10304/60000]\n",
            "loss: 1090.375000  [10944/60000]\n",
            "loss: 1428.255249  [11584/60000]\n",
            "loss: 4200.675781  [12224/60000]\n",
            "loss: 899.293457  [12864/60000]\n",
            "loss: 1227.240967  [13504/60000]\n",
            "loss: 403.768616  [14144/60000]\n",
            "loss: 1555.781982  [14784/60000]\n",
            "loss: 326.834961  [15424/60000]\n",
            "loss: 819.047791  [16064/60000]\n",
            "loss: 411.153687  [16704/60000]\n",
            "loss: 1276.589355  [17344/60000]\n",
            "loss: 1326.948486  [17984/60000]\n",
            "loss: 590.423218  [18624/60000]\n",
            "loss: 1119.364624  [19264/60000]\n",
            "loss: 1719.873901  [19904/60000]\n",
            "loss: 954.612183  [20544/60000]\n",
            "loss: 708.095886  [21184/60000]\n",
            "loss: 4139.676270  [21824/60000]\n",
            "loss: 1071.519897  [22464/60000]\n",
            "loss: 1729.807373  [23104/60000]\n",
            "loss: 688.239929  [23744/60000]\n",
            "loss: 1912.672119  [24384/60000]\n",
            "loss: 884.052612  [25024/60000]\n",
            "loss: 1278.606567  [25664/60000]\n",
            "loss: 2043.316040  [26304/60000]\n",
            "loss: 1033.433594  [26944/60000]\n",
            "loss: 1704.281494  [27584/60000]\n",
            "loss: 741.274048  [28224/60000]\n",
            "loss: 996.299561  [28864/60000]\n",
            "loss: 1937.785645  [29504/60000]\n",
            "loss: 2036.705078  [30144/60000]\n",
            "loss: 646.023743  [30784/60000]\n",
            "loss: 500.234741  [31424/60000]\n",
            "loss: 704.732788  [32064/60000]\n",
            "loss: 757.553528  [32704/60000]\n",
            "loss: 534.335938  [33344/60000]\n",
            "loss: 1401.731201  [33984/60000]\n",
            "loss: 377.139282  [34624/60000]\n",
            "loss: 464.005249  [35264/60000]\n",
            "loss: 793.922058  [35904/60000]\n",
            "loss: 1069.512573  [36544/60000]\n",
            "loss: 2064.598633  [37184/60000]\n",
            "loss: 373.553284  [37824/60000]\n",
            "loss: 1108.439575  [38464/60000]\n",
            "loss: 1824.878662  [39104/60000]\n",
            "loss: 1559.236938  [39744/60000]\n",
            "loss: 2356.066650  [40384/60000]\n",
            "loss: 1732.395752  [41024/60000]\n",
            "loss: 913.208435  [41664/60000]\n",
            "loss: 1825.821289  [42304/60000]\n",
            "loss: 1342.531616  [42944/60000]\n",
            "loss: 739.986938  [43584/60000]\n",
            "loss: 2012.221680  [44224/60000]\n",
            "loss: 983.173157  [44864/60000]\n",
            "loss: 1155.814819  [45504/60000]\n",
            "loss: 808.809021  [46144/60000]\n",
            "loss: 1206.107788  [46784/60000]\n",
            "loss: 447.856903  [47424/60000]\n",
            "loss: 942.421387  [48064/60000]\n",
            "loss: 754.832275  [48704/60000]\n",
            "loss: 551.748718  [49344/60000]\n",
            "loss: 1292.547607  [49984/60000]\n",
            "loss: 889.604309  [50624/60000]\n",
            "loss: 1913.080811  [51264/60000]\n",
            "loss: 2806.959717  [51904/60000]\n",
            "loss: 3325.565186  [52544/60000]\n",
            "loss: 1187.267822  [53184/60000]\n",
            "loss: 1069.520508  [53824/60000]\n",
            "loss: 1177.131592  [54464/60000]\n",
            "loss: 459.105133  [55104/60000]\n",
            "loss: 746.993408  [55744/60000]\n",
            "loss: 824.707031  [56384/60000]\n",
            "loss: 1024.107422  [57024/60000]\n",
            "loss: 1092.593018  [57664/60000]\n",
            "loss: 357.625610  [58304/60000]\n",
            "loss: 792.266663  [58944/60000]\n",
            "loss: 1502.189819  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.1%, Avg loss: 1726.575051 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1255.271240  [   64/60000]\n",
            "loss: 653.423340  [  704/60000]\n",
            "loss: 242.680038  [ 1344/60000]\n",
            "loss: 1923.755981  [ 1984/60000]\n",
            "loss: 1198.904053  [ 2624/60000]\n",
            "loss: 954.859741  [ 3264/60000]\n",
            "loss: 1328.689941  [ 3904/60000]\n",
            "loss: 448.838806  [ 4544/60000]\n",
            "loss: 3122.978271  [ 5184/60000]\n",
            "loss: 1699.554688  [ 5824/60000]\n",
            "loss: 444.822906  [ 6464/60000]\n",
            "loss: 554.218506  [ 7104/60000]\n",
            "loss: 3267.192871  [ 7744/60000]\n",
            "loss: 2277.032959  [ 8384/60000]\n",
            "loss: 1107.063965  [ 9024/60000]\n",
            "loss: 1731.004883  [ 9664/60000]\n",
            "loss: 1396.274902  [10304/60000]\n",
            "loss: 504.195831  [10944/60000]\n",
            "loss: 1044.231445  [11584/60000]\n",
            "loss: 2016.148682  [12224/60000]\n",
            "loss: 1360.214844  [12864/60000]\n",
            "loss: 1781.512573  [13504/60000]\n",
            "loss: 1660.205811  [14144/60000]\n",
            "loss: 920.206543  [14784/60000]\n",
            "loss: 231.143738  [15424/60000]\n",
            "loss: 809.228882  [16064/60000]\n",
            "loss: 1250.362427  [16704/60000]\n",
            "loss: 1441.612793  [17344/60000]\n",
            "loss: 1384.470703  [17984/60000]\n",
            "loss: 706.426575  [18624/60000]\n",
            "loss: 1325.613403  [19264/60000]\n",
            "loss: 1323.724854  [19904/60000]\n",
            "loss: 589.020081  [20544/60000]\n",
            "loss: 853.207825  [21184/60000]\n",
            "loss: 4112.232422  [21824/60000]\n",
            "loss: 643.519958  [22464/60000]\n",
            "loss: 4261.043945  [23104/60000]\n",
            "loss: 805.282593  [23744/60000]\n",
            "loss: 1411.035645  [24384/60000]\n",
            "loss: 755.139526  [25024/60000]\n",
            "loss: 816.747192  [25664/60000]\n",
            "loss: 1916.269531  [26304/60000]\n",
            "loss: 979.090881  [26944/60000]\n",
            "loss: 1924.777832  [27584/60000]\n",
            "loss: 925.353027  [28224/60000]\n",
            "loss: 726.270325  [28864/60000]\n",
            "loss: 1367.619873  [29504/60000]\n",
            "loss: 1322.775391  [30144/60000]\n",
            "loss: 1052.933472  [30784/60000]\n",
            "loss: 428.026367  [31424/60000]\n",
            "loss: 1210.919678  [32064/60000]\n",
            "loss: 895.424927  [32704/60000]\n",
            "loss: 667.869385  [33344/60000]\n",
            "loss: 1992.964478  [33984/60000]\n",
            "loss: 533.447510  [34624/60000]\n",
            "loss: 583.894653  [35264/60000]\n",
            "loss: 2012.188721  [35904/60000]\n",
            "loss: 1202.854004  [36544/60000]\n",
            "loss: 2487.158203  [37184/60000]\n",
            "loss: 541.851929  [37824/60000]\n",
            "loss: 1031.182129  [38464/60000]\n",
            "loss: 1122.524536  [39104/60000]\n",
            "loss: 1043.429199  [39744/60000]\n",
            "loss: 1059.842163  [40384/60000]\n",
            "loss: 620.482910  [41024/60000]\n",
            "loss: 1399.899780  [41664/60000]\n",
            "loss: 1101.480957  [42304/60000]\n",
            "loss: 807.817139  [42944/60000]\n",
            "loss: 790.970459  [43584/60000]\n",
            "loss: 2388.895752  [44224/60000]\n",
            "loss: 1610.887085  [44864/60000]\n",
            "loss: 576.732178  [45504/60000]\n",
            "loss: 738.469543  [46144/60000]\n",
            "loss: 1937.146484  [46784/60000]\n",
            "loss: 397.068542  [47424/60000]\n",
            "loss: 1389.640869  [48064/60000]\n",
            "loss: 715.530396  [48704/60000]\n",
            "loss: 747.133545  [49344/60000]\n",
            "loss: 1540.698730  [49984/60000]\n",
            "loss: 646.194641  [50624/60000]\n",
            "loss: 1993.307495  [51264/60000]\n",
            "loss: 3627.407227  [51904/60000]\n",
            "loss: 2572.627686  [52544/60000]\n",
            "loss: 691.146301  [53184/60000]\n",
            "loss: 707.706665  [53824/60000]\n",
            "loss: 2024.773193  [54464/60000]\n",
            "loss: 407.315186  [55104/60000]\n",
            "loss: 997.818970  [55744/60000]\n",
            "loss: 557.655762  [56384/60000]\n",
            "loss: 1125.891357  [57024/60000]\n",
            "loss: 1136.206543  [57664/60000]\n",
            "loss: 274.430481  [58304/60000]\n",
            "loss: 667.226807  [58944/60000]\n",
            "loss: 2120.378418  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 3123.529690 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2446.531982  [   64/60000]\n",
            "loss: 871.675293  [  704/60000]\n",
            "loss: 459.306641  [ 1344/60000]\n",
            "loss: 548.988464  [ 1984/60000]\n",
            "loss: 1176.798462  [ 2624/60000]\n",
            "loss: 832.912231  [ 3264/60000]\n",
            "loss: 1823.422119  [ 3904/60000]\n",
            "loss: 918.425415  [ 4544/60000]\n",
            "loss: 1581.079834  [ 5184/60000]\n",
            "loss: 519.413208  [ 5824/60000]\n",
            "loss: 320.954956  [ 6464/60000]\n",
            "loss: 870.423828  [ 7104/60000]\n",
            "loss: 1953.361816  [ 7744/60000]\n",
            "loss: 828.744507  [ 8384/60000]\n",
            "loss: 706.456299  [ 9024/60000]\n",
            "loss: 814.942261  [ 9664/60000]\n",
            "loss: 1849.577515  [10304/60000]\n",
            "loss: 2778.397217  [10944/60000]\n",
            "loss: 1077.127563  [11584/60000]\n",
            "loss: 1974.210205  [12224/60000]\n",
            "loss: 2626.340088  [12864/60000]\n",
            "loss: 1500.008057  [13504/60000]\n",
            "loss: 339.812866  [14144/60000]\n",
            "loss: 925.377930  [14784/60000]\n",
            "loss: 446.317871  [15424/60000]\n",
            "loss: 1123.751953  [16064/60000]\n",
            "loss: 829.269836  [16704/60000]\n",
            "loss: 1360.867432  [17344/60000]\n",
            "loss: 798.158081  [17984/60000]\n",
            "loss: 613.242004  [18624/60000]\n",
            "loss: 1665.307861  [19264/60000]\n",
            "loss: 1401.403320  [19904/60000]\n",
            "loss: 508.410187  [20544/60000]\n",
            "loss: 1036.184814  [21184/60000]\n",
            "loss: 2831.229980  [21824/60000]\n",
            "loss: 2123.432617  [22464/60000]\n",
            "loss: 1001.340759  [23104/60000]\n",
            "loss: 786.412048  [23744/60000]\n",
            "loss: 1264.887939  [24384/60000]\n",
            "loss: 879.031860  [25024/60000]\n",
            "loss: 1302.724487  [25664/60000]\n",
            "loss: 1820.944946  [26304/60000]\n",
            "loss: 990.856567  [26944/60000]\n",
            "loss: 1903.718750  [27584/60000]\n",
            "loss: 512.235779  [28224/60000]\n",
            "loss: 384.493561  [28864/60000]\n",
            "loss: 1972.002930  [29504/60000]\n",
            "loss: 1047.425537  [30144/60000]\n",
            "loss: 1069.586182  [30784/60000]\n",
            "loss: 204.245392  [31424/60000]\n",
            "loss: 603.103882  [32064/60000]\n",
            "loss: 801.557617  [32704/60000]\n",
            "loss: 840.427612  [33344/60000]\n",
            "loss: 2313.901367  [33984/60000]\n",
            "loss: 301.876373  [34624/60000]\n",
            "loss: 389.185242  [35264/60000]\n",
            "loss: 1052.820679  [35904/60000]\n",
            "loss: 1359.680542  [36544/60000]\n",
            "loss: 1947.016113  [37184/60000]\n",
            "loss: 703.745422  [37824/60000]\n",
            "loss: 1454.851929  [38464/60000]\n",
            "loss: 1882.029541  [39104/60000]\n",
            "loss: 1638.677490  [39744/60000]\n",
            "loss: 2247.457031  [40384/60000]\n",
            "loss: 1277.791626  [41024/60000]\n",
            "loss: 945.411987  [41664/60000]\n",
            "loss: 1556.245972  [42304/60000]\n",
            "loss: 812.754395  [42944/60000]\n",
            "loss: 2174.176270  [43584/60000]\n",
            "loss: 1820.135620  [44224/60000]\n",
            "loss: 1310.940918  [44864/60000]\n",
            "loss: 1136.314697  [45504/60000]\n",
            "loss: 785.140381  [46144/60000]\n",
            "loss: 1187.309326  [46784/60000]\n",
            "loss: 1205.599854  [47424/60000]\n",
            "loss: 1120.279053  [48064/60000]\n",
            "loss: 677.787231  [48704/60000]\n",
            "loss: 1066.403442  [49344/60000]\n",
            "loss: 1252.522705  [49984/60000]\n",
            "loss: 473.668213  [50624/60000]\n",
            "loss: 1276.748535  [51264/60000]\n",
            "loss: 2592.370361  [51904/60000]\n",
            "loss: 3634.337891  [52544/60000]\n",
            "loss: 1502.166992  [53184/60000]\n",
            "loss: 495.423767  [53824/60000]\n",
            "loss: 2289.883057  [54464/60000]\n",
            "loss: 795.650024  [55104/60000]\n",
            "loss: 673.989136  [55744/60000]\n",
            "loss: 853.126648  [56384/60000]\n",
            "loss: 1492.119385  [57024/60000]\n",
            "loss: 1250.093262  [57664/60000]\n",
            "loss: 403.781860  [58304/60000]\n",
            "loss: 629.011902  [58944/60000]\n",
            "loss: 2541.156738  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.9%, Avg loss: 1805.035045 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1315.865479  [   64/60000]\n",
            "loss: 939.617432  [  704/60000]\n",
            "loss: 367.628113  [ 1344/60000]\n",
            "loss: 685.425415  [ 1984/60000]\n",
            "loss: 1151.461060  [ 2624/60000]\n",
            "loss: 888.996155  [ 3264/60000]\n",
            "loss: 1093.665527  [ 3904/60000]\n",
            "loss: 702.964355  [ 4544/60000]\n",
            "loss: 1175.948364  [ 5184/60000]\n",
            "loss: 1650.454956  [ 5824/60000]\n",
            "loss: 454.010223  [ 6464/60000]\n",
            "loss: 2197.791504  [ 7104/60000]\n",
            "loss: 2363.077637  [ 7744/60000]\n",
            "loss: 1340.848389  [ 8384/60000]\n",
            "loss: 854.451111  [ 9024/60000]\n",
            "loss: 2387.750488  [ 9664/60000]\n",
            "loss: 533.966431  [10304/60000]\n",
            "loss: 554.101685  [10944/60000]\n",
            "loss: 1029.255981  [11584/60000]\n",
            "loss: 4807.234375  [12224/60000]\n",
            "loss: 2036.955933  [12864/60000]\n",
            "loss: 1338.481934  [13504/60000]\n",
            "loss: 347.254608  [14144/60000]\n",
            "loss: 1173.211548  [14784/60000]\n",
            "loss: 377.456726  [15424/60000]\n",
            "loss: 923.966431  [16064/60000]\n",
            "loss: 770.011414  [16704/60000]\n",
            "loss: 1421.919189  [17344/60000]\n",
            "loss: 1148.701172  [17984/60000]\n",
            "loss: 868.058411  [18624/60000]\n",
            "loss: 1378.408569  [19264/60000]\n",
            "loss: 1686.358643  [19904/60000]\n",
            "loss: 356.130981  [20544/60000]\n",
            "loss: 1040.489868  [21184/60000]\n",
            "loss: 2214.087891  [21824/60000]\n",
            "loss: 1095.919434  [22464/60000]\n",
            "loss: 647.529663  [23104/60000]\n",
            "loss: 666.906616  [23744/60000]\n",
            "loss: 1968.947144  [24384/60000]\n",
            "loss: 904.612183  [25024/60000]\n",
            "loss: 1465.889893  [25664/60000]\n",
            "loss: 1352.425781  [26304/60000]\n",
            "loss: 1115.357178  [26944/60000]\n",
            "loss: 1519.026245  [27584/60000]\n",
            "loss: 1447.363159  [28224/60000]\n",
            "loss: 367.529327  [28864/60000]\n",
            "loss: 1741.675171  [29504/60000]\n",
            "loss: 2294.625977  [30144/60000]\n",
            "loss: 757.684204  [30784/60000]\n",
            "loss: 592.719604  [31424/60000]\n",
            "loss: 1797.537354  [32064/60000]\n",
            "loss: 780.116882  [32704/60000]\n",
            "loss: 730.054565  [33344/60000]\n",
            "loss: 1571.566650  [33984/60000]\n",
            "loss: 412.490021  [34624/60000]\n",
            "loss: 340.750641  [35264/60000]\n",
            "loss: 687.763550  [35904/60000]\n",
            "loss: 1074.308838  [36544/60000]\n",
            "loss: 2655.451904  [37184/60000]\n",
            "loss: 485.485168  [37824/60000]\n",
            "loss: 1596.084229  [38464/60000]\n",
            "loss: 1334.967041  [39104/60000]\n",
            "loss: 1395.558350  [39744/60000]\n",
            "loss: 1274.829590  [40384/60000]\n",
            "loss: 1954.452148  [41024/60000]\n",
            "loss: 4011.616699  [41664/60000]\n",
            "loss: 1043.778564  [42304/60000]\n",
            "loss: 3447.270508  [42944/60000]\n",
            "loss: 1786.773071  [43584/60000]\n",
            "loss: 1466.326904  [44224/60000]\n",
            "loss: 1473.116699  [44864/60000]\n",
            "loss: 874.655762  [45504/60000]\n",
            "loss: 1427.559326  [46144/60000]\n",
            "loss: 1131.473633  [46784/60000]\n",
            "loss: 796.393188  [47424/60000]\n",
            "loss: 828.375610  [48064/60000]\n",
            "loss: 946.685181  [48704/60000]\n",
            "loss: 600.747375  [49344/60000]\n",
            "loss: 1755.072266  [49984/60000]\n",
            "loss: 868.102905  [50624/60000]\n",
            "loss: 1219.682495  [51264/60000]\n",
            "loss: 2009.970093  [51904/60000]\n",
            "loss: 1900.866577  [52544/60000]\n",
            "loss: 1816.263550  [53184/60000]\n",
            "loss: 840.682617  [53824/60000]\n",
            "loss: 790.389465  [54464/60000]\n",
            "loss: 1120.345947  [55104/60000]\n",
            "loss: 773.121338  [55744/60000]\n",
            "loss: 547.348206  [56384/60000]\n",
            "loss: 948.720886  [57024/60000]\n",
            "loss: 1404.383057  [57664/60000]\n",
            "loss: 362.075073  [58304/60000]\n",
            "loss: 580.151489  [58944/60000]\n",
            "loss: 1743.454102  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.6%, Avg loss: 2699.766248 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2056.083740  [   64/60000]\n",
            "loss: 681.642700  [  704/60000]\n",
            "loss: 373.721222  [ 1344/60000]\n",
            "loss: 643.907593  [ 1984/60000]\n",
            "loss: 1050.259521  [ 2624/60000]\n",
            "loss: 844.623413  [ 3264/60000]\n",
            "loss: 1179.322510  [ 3904/60000]\n",
            "loss: 821.079773  [ 4544/60000]\n",
            "loss: 2520.735840  [ 5184/60000]\n",
            "loss: 407.688385  [ 5824/60000]\n",
            "loss: 457.893280  [ 6464/60000]\n",
            "loss: 839.137146  [ 7104/60000]\n",
            "loss: 1731.262939  [ 7744/60000]\n",
            "loss: 1149.098511  [ 8384/60000]\n",
            "loss: 1028.437012  [ 9024/60000]\n",
            "loss: 666.369751  [ 9664/60000]\n",
            "loss: 761.183350  [10304/60000]\n",
            "loss: 512.669861  [10944/60000]\n",
            "loss: 827.936951  [11584/60000]\n",
            "loss: 1936.000854  [12224/60000]\n",
            "loss: 1166.484497  [12864/60000]\n",
            "loss: 1818.201172  [13504/60000]\n",
            "loss: 323.930969  [14144/60000]\n",
            "loss: 870.096313  [14784/60000]\n",
            "loss: 709.922363  [15424/60000]\n",
            "loss: 1762.794312  [16064/60000]\n",
            "loss: 571.212891  [16704/60000]\n",
            "loss: 1186.763916  [17344/60000]\n",
            "loss: 887.920593  [17984/60000]\n",
            "loss: 835.716309  [18624/60000]\n",
            "loss: 1238.812012  [19264/60000]\n",
            "loss: 1905.742798  [19904/60000]\n",
            "loss: 409.184052  [20544/60000]\n",
            "loss: 873.559387  [21184/60000]\n",
            "loss: 3367.678467  [21824/60000]\n",
            "loss: 669.810303  [22464/60000]\n",
            "loss: 690.238647  [23104/60000]\n",
            "loss: 728.574219  [23744/60000]\n",
            "loss: 1250.078613  [24384/60000]\n",
            "loss: 788.356140  [25024/60000]\n",
            "loss: 972.471924  [25664/60000]\n",
            "loss: 1704.813232  [26304/60000]\n",
            "loss: 1030.336548  [26944/60000]\n",
            "loss: 1635.371094  [27584/60000]\n",
            "loss: 769.282104  [28224/60000]\n",
            "loss: 398.725983  [28864/60000]\n",
            "loss: 2006.034546  [29504/60000]\n",
            "loss: 2288.703857  [30144/60000]\n",
            "loss: 880.682373  [30784/60000]\n",
            "loss: 253.839249  [31424/60000]\n",
            "loss: 778.957947  [32064/60000]\n",
            "loss: 1407.422363  [32704/60000]\n",
            "loss: 595.721130  [33344/60000]\n",
            "loss: 1572.049072  [33984/60000]\n",
            "loss: 810.377991  [34624/60000]\n",
            "loss: 540.485596  [35264/60000]\n",
            "loss: 616.311157  [35904/60000]\n",
            "loss: 1590.698608  [36544/60000]\n",
            "loss: 3031.145752  [37184/60000]\n",
            "loss: 374.544617  [37824/60000]\n",
            "loss: 1038.823486  [38464/60000]\n",
            "loss: 1680.851685  [39104/60000]\n",
            "loss: 1276.900513  [39744/60000]\n",
            "loss: 1008.383789  [40384/60000]\n",
            "loss: 1368.147461  [41024/60000]\n",
            "loss: 1628.482422  [41664/60000]\n",
            "loss: 1830.549805  [42304/60000]\n",
            "loss: 512.993591  [42944/60000]\n",
            "loss: 839.007019  [43584/60000]\n",
            "loss: 2333.925049  [44224/60000]\n",
            "loss: 1440.328125  [44864/60000]\n",
            "loss: 780.921692  [45504/60000]\n",
            "loss: 1042.097534  [46144/60000]\n",
            "loss: 1534.467041  [46784/60000]\n",
            "loss: 851.458984  [47424/60000]\n",
            "loss: 1000.153015  [48064/60000]\n",
            "loss: 594.346313  [48704/60000]\n",
            "loss: 1245.875977  [49344/60000]\n",
            "loss: 1159.563477  [49984/60000]\n",
            "loss: 722.088135  [50624/60000]\n",
            "loss: 896.081421  [51264/60000]\n",
            "loss: 3318.193359  [51904/60000]\n",
            "loss: 3353.877197  [52544/60000]\n",
            "loss: 1222.186035  [53184/60000]\n",
            "loss: 711.689026  [53824/60000]\n",
            "loss: 648.234253  [54464/60000]\n",
            "loss: 753.179993  [55104/60000]\n",
            "loss: 765.078247  [55744/60000]\n",
            "loss: 973.036804  [56384/60000]\n",
            "loss: 984.157715  [57024/60000]\n",
            "loss: 1621.296021  [57664/60000]\n",
            "loss: 291.912781  [58304/60000]\n",
            "loss: 637.970093  [58944/60000]\n",
            "loss: 2062.993408  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 2214.048317 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1732.416504  [   64/60000]\n",
            "loss: 648.635071  [  704/60000]\n",
            "loss: 354.545349  [ 1344/60000]\n",
            "loss: 699.128174  [ 1984/60000]\n",
            "loss: 1012.616821  [ 2624/60000]\n",
            "loss: 753.887451  [ 3264/60000]\n",
            "loss: 1174.309570  [ 3904/60000]\n",
            "loss: 581.373108  [ 4544/60000]\n",
            "loss: 3690.999268  [ 5184/60000]\n",
            "loss: 458.876862  [ 5824/60000]\n",
            "loss: 418.753113  [ 6464/60000]\n",
            "loss: 774.879639  [ 7104/60000]\n",
            "loss: 3132.217041  [ 7744/60000]\n",
            "loss: 899.469421  [ 8384/60000]\n",
            "loss: 759.338745  [ 9024/60000]\n",
            "loss: 3812.900391  [ 9664/60000]\n",
            "loss: 729.012085  [10304/60000]\n",
            "loss: 535.055542  [10944/60000]\n",
            "loss: 1167.751099  [11584/60000]\n",
            "loss: 1569.244751  [12224/60000]\n",
            "loss: 1313.435303  [12864/60000]\n",
            "loss: 1925.502563  [13504/60000]\n",
            "loss: 433.448822  [14144/60000]\n",
            "loss: 1147.351440  [14784/60000]\n",
            "loss: 202.006287  [15424/60000]\n",
            "loss: 805.897949  [16064/60000]\n",
            "loss: 1247.806030  [16704/60000]\n",
            "loss: 1315.370728  [17344/60000]\n",
            "loss: 862.959229  [17984/60000]\n",
            "loss: 773.466553  [18624/60000]\n",
            "loss: 1193.817871  [19264/60000]\n",
            "loss: 1313.975708  [19904/60000]\n",
            "loss: 417.460754  [20544/60000]\n",
            "loss: 676.531128  [21184/60000]\n",
            "loss: 2943.628418  [21824/60000]\n",
            "loss: 930.550476  [22464/60000]\n",
            "loss: 1693.653076  [23104/60000]\n",
            "loss: 860.349365  [23744/60000]\n",
            "loss: 1220.058594  [24384/60000]\n",
            "loss: 894.694031  [25024/60000]\n",
            "loss: 1002.828430  [25664/60000]\n",
            "loss: 1770.134521  [26304/60000]\n",
            "loss: 1007.280884  [26944/60000]\n",
            "loss: 1653.735107  [27584/60000]\n",
            "loss: 669.850952  [28224/60000]\n",
            "loss: 309.862579  [28864/60000]\n",
            "loss: 1126.424927  [29504/60000]\n",
            "loss: 1200.936279  [30144/60000]\n",
            "loss: 960.865112  [30784/60000]\n",
            "loss: 638.659302  [31424/60000]\n",
            "loss: 1236.867188  [32064/60000]\n",
            "loss: 1145.134644  [32704/60000]\n",
            "loss: 406.117737  [33344/60000]\n",
            "loss: 945.741333  [33984/60000]\n",
            "loss: 393.571045  [34624/60000]\n",
            "loss: 456.919373  [35264/60000]\n",
            "loss: 1094.629883  [35904/60000]\n",
            "loss: 696.726501  [36544/60000]\n",
            "loss: 2747.485352  [37184/60000]\n",
            "loss: 366.061829  [37824/60000]\n",
            "loss: 1465.822510  [38464/60000]\n",
            "loss: 1179.412476  [39104/60000]\n",
            "loss: 918.314026  [39744/60000]\n",
            "loss: 2633.692139  [40384/60000]\n",
            "loss: 1852.786621  [41024/60000]\n",
            "loss: 904.457031  [41664/60000]\n",
            "loss: 1625.638672  [42304/60000]\n",
            "loss: 1308.958130  [42944/60000]\n",
            "loss: 1897.139893  [43584/60000]\n",
            "loss: 1045.000610  [44224/60000]\n",
            "loss: 1519.321533  [44864/60000]\n",
            "loss: 878.503113  [45504/60000]\n",
            "loss: 966.514893  [46144/60000]\n",
            "loss: 946.969482  [46784/60000]\n",
            "loss: 892.679077  [47424/60000]\n",
            "loss: 994.153320  [48064/60000]\n",
            "loss: 642.020874  [48704/60000]\n",
            "loss: 624.864990  [49344/60000]\n",
            "loss: 1544.070068  [49984/60000]\n",
            "loss: 706.706970  [50624/60000]\n",
            "loss: 1035.594360  [51264/60000]\n",
            "loss: 3153.764893  [51904/60000]\n",
            "loss: 867.144653  [52544/60000]\n",
            "loss: 1231.604248  [53184/60000]\n",
            "loss: 979.115295  [53824/60000]\n",
            "loss: 2718.001953  [54464/60000]\n",
            "loss: 367.838196  [55104/60000]\n",
            "loss: 1211.081665  [55744/60000]\n",
            "loss: 1452.282959  [56384/60000]\n",
            "loss: 862.821350  [57024/60000]\n",
            "loss: 952.857422  [57664/60000]\n",
            "loss: 576.895020  [58304/60000]\n",
            "loss: 611.239990  [58944/60000]\n",
            "loss: 2053.827148  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 1929.146426 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1162.511230  [   64/60000]\n",
            "loss: 1328.735107  [  704/60000]\n",
            "loss: 580.546143  [ 1344/60000]\n",
            "loss: 701.528809  [ 1984/60000]\n",
            "loss: 1258.158325  [ 2624/60000]\n",
            "loss: 759.026611  [ 3264/60000]\n",
            "loss: 1551.383057  [ 3904/60000]\n",
            "loss: 517.298950  [ 4544/60000]\n",
            "loss: 2388.738770  [ 5184/60000]\n",
            "loss: 491.459595  [ 5824/60000]\n",
            "loss: 380.275146  [ 6464/60000]\n",
            "loss: 2143.403809  [ 7104/60000]\n",
            "loss: 1779.276855  [ 7744/60000]\n",
            "loss: 1018.905273  [ 8384/60000]\n",
            "loss: 841.433960  [ 9024/60000]\n",
            "loss: 768.399170  [ 9664/60000]\n",
            "loss: 1348.653809  [10304/60000]\n",
            "loss: 558.201416  [10944/60000]\n",
            "loss: 1398.455322  [11584/60000]\n",
            "loss: 2065.308105  [12224/60000]\n",
            "loss: 3497.993164  [12864/60000]\n",
            "loss: 1078.149048  [13504/60000]\n",
            "loss: 1058.505371  [14144/60000]\n",
            "loss: 1072.638184  [14784/60000]\n",
            "loss: 921.121948  [15424/60000]\n",
            "loss: 826.942261  [16064/60000]\n",
            "loss: 710.980530  [16704/60000]\n",
            "loss: 1471.573364  [17344/60000]\n",
            "loss: 975.444946  [17984/60000]\n",
            "loss: 807.068481  [18624/60000]\n",
            "loss: 1618.053467  [19264/60000]\n",
            "loss: 1409.190918  [19904/60000]\n",
            "loss: 470.151184  [20544/60000]\n",
            "loss: 633.340088  [21184/60000]\n",
            "loss: 2068.823975  [21824/60000]\n",
            "loss: 868.501831  [22464/60000]\n",
            "loss: 1126.830688  [23104/60000]\n",
            "loss: 803.826294  [23744/60000]\n",
            "loss: 1034.457031  [24384/60000]\n",
            "loss: 737.076904  [25024/60000]\n",
            "loss: 1029.325684  [25664/60000]\n",
            "loss: 1660.960083  [26304/60000]\n",
            "loss: 1038.755127  [26944/60000]\n",
            "loss: 1809.793457  [27584/60000]\n",
            "loss: 752.218384  [28224/60000]\n",
            "loss: 337.135010  [28864/60000]\n",
            "loss: 1572.756592  [29504/60000]\n",
            "loss: 1709.826904  [30144/60000]\n",
            "loss: 1108.809082  [30784/60000]\n",
            "loss: 224.287750  [31424/60000]\n",
            "loss: 822.803467  [32064/60000]\n",
            "loss: 1261.183838  [32704/60000]\n",
            "loss: 352.936646  [33344/60000]\n",
            "loss: 1023.742188  [33984/60000]\n",
            "loss: 548.641357  [34624/60000]\n",
            "loss: 471.904205  [35264/60000]\n",
            "loss: 748.637817  [35904/60000]\n",
            "loss: 895.521362  [36544/60000]\n",
            "loss: 2539.488770  [37184/60000]\n",
            "loss: 436.490570  [37824/60000]\n",
            "loss: 1541.234741  [38464/60000]\n",
            "loss: 989.655762  [39104/60000]\n",
            "loss: 2048.648926  [39744/60000]\n",
            "loss: 1080.380127  [40384/60000]\n",
            "loss: 1669.829956  [41024/60000]\n",
            "loss: 1271.632568  [41664/60000]\n",
            "loss: 1253.086182  [42304/60000]\n",
            "loss: 1888.318726  [42944/60000]\n",
            "loss: 545.700867  [43584/60000]\n",
            "loss: 1965.620972  [44224/60000]\n",
            "loss: 1351.835449  [44864/60000]\n",
            "loss: 619.757874  [45504/60000]\n",
            "loss: 880.475708  [46144/60000]\n",
            "loss: 1516.750977  [46784/60000]\n",
            "loss: 840.726685  [47424/60000]\n",
            "loss: 1136.233032  [48064/60000]\n",
            "loss: 702.514343  [48704/60000]\n",
            "loss: 635.780029  [49344/60000]\n",
            "loss: 1070.695801  [49984/60000]\n",
            "loss: 794.676147  [50624/60000]\n",
            "loss: 1341.981201  [51264/60000]\n",
            "loss: 1746.476562  [51904/60000]\n",
            "loss: 921.501648  [52544/60000]\n",
            "loss: 2433.693848  [53184/60000]\n",
            "loss: 1056.086914  [53824/60000]\n",
            "loss: 1944.246826  [54464/60000]\n",
            "loss: 556.335571  [55104/60000]\n",
            "loss: 682.106995  [55744/60000]\n",
            "loss: 603.717651  [56384/60000]\n",
            "loss: 833.051147  [57024/60000]\n",
            "loss: 1708.861206  [57664/60000]\n",
            "loss: 424.746979  [58304/60000]\n",
            "loss: 666.084717  [58944/60000]\n",
            "loss: 2034.988892  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 2966.600249 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2265.871094  [   64/60000]\n",
            "loss: 1004.750366  [  704/60000]\n",
            "loss: 709.524780  [ 1344/60000]\n",
            "loss: 890.667236  [ 1984/60000]\n",
            "loss: 1094.577393  [ 2624/60000]\n",
            "loss: 816.326477  [ 3264/60000]\n",
            "loss: 1213.192505  [ 3904/60000]\n",
            "loss: 734.385132  [ 4544/60000]\n",
            "loss: 1024.620483  [ 5184/60000]\n",
            "loss: 2080.417969  [ 5824/60000]\n",
            "loss: 505.337616  [ 6464/60000]\n",
            "loss: 1273.940186  [ 7104/60000]\n",
            "loss: 2592.833496  [ 7744/60000]\n",
            "loss: 1326.034180  [ 8384/60000]\n",
            "loss: 1010.256531  [ 9024/60000]\n",
            "loss: 971.836365  [ 9664/60000]\n",
            "loss: 815.425110  [10304/60000]\n",
            "loss: 831.317627  [10944/60000]\n",
            "loss: 1062.132568  [11584/60000]\n",
            "loss: 3090.695557  [12224/60000]\n",
            "loss: 983.511841  [12864/60000]\n",
            "loss: 1209.809326  [13504/60000]\n",
            "loss: 352.632446  [14144/60000]\n",
            "loss: 801.241028  [14784/60000]\n",
            "loss: 550.162720  [15424/60000]\n",
            "loss: 637.310913  [16064/60000]\n",
            "loss: 1250.813232  [16704/60000]\n",
            "loss: 1153.336426  [17344/60000]\n",
            "loss: 1168.100220  [17984/60000]\n",
            "loss: 576.310181  [18624/60000]\n",
            "loss: 612.060547  [19264/60000]\n",
            "loss: 1736.042114  [19904/60000]\n",
            "loss: 461.293518  [20544/60000]\n",
            "loss: 1037.347656  [21184/60000]\n",
            "loss: 2216.649170  [21824/60000]\n",
            "loss: 975.850586  [22464/60000]\n",
            "loss: 900.195679  [23104/60000]\n",
            "loss: 727.940308  [23744/60000]\n",
            "loss: 2101.274902  [24384/60000]\n",
            "loss: 650.937073  [25024/60000]\n",
            "loss: 1159.361328  [25664/60000]\n",
            "loss: 2112.463867  [26304/60000]\n",
            "loss: 1247.507080  [26944/60000]\n",
            "loss: 1673.593384  [27584/60000]\n",
            "loss: 593.640076  [28224/60000]\n",
            "loss: 462.734009  [28864/60000]\n",
            "loss: 2169.001953  [29504/60000]\n",
            "loss: 1967.381958  [30144/60000]\n",
            "loss: 875.415039  [30784/60000]\n",
            "loss: 411.990082  [31424/60000]\n",
            "loss: 1085.742310  [32064/60000]\n",
            "loss: 665.699585  [32704/60000]\n",
            "loss: 724.600586  [33344/60000]\n",
            "loss: 1352.426270  [33984/60000]\n",
            "loss: 458.597595  [34624/60000]\n",
            "loss: 427.476440  [35264/60000]\n",
            "loss: 800.105957  [35904/60000]\n",
            "loss: 755.385803  [36544/60000]\n",
            "loss: 2521.947021  [37184/60000]\n",
            "loss: 390.686279  [37824/60000]\n",
            "loss: 1439.756592  [38464/60000]\n",
            "loss: 619.775818  [39104/60000]\n",
            "loss: 1413.803711  [39744/60000]\n",
            "loss: 1260.019653  [40384/60000]\n",
            "loss: 1208.467529  [41024/60000]\n",
            "loss: 1129.388550  [41664/60000]\n",
            "loss: 1660.114258  [42304/60000]\n",
            "loss: 628.129883  [42944/60000]\n",
            "loss: 1963.727173  [43584/60000]\n",
            "loss: 2518.889648  [44224/60000]\n",
            "loss: 1167.121704  [44864/60000]\n",
            "loss: 711.102661  [45504/60000]\n",
            "loss: 800.904114  [46144/60000]\n",
            "loss: 1053.924805  [46784/60000]\n",
            "loss: 627.527466  [47424/60000]\n",
            "loss: 1091.219604  [48064/60000]\n",
            "loss: 799.144287  [48704/60000]\n",
            "loss: 681.404419  [49344/60000]\n",
            "loss: 1333.720337  [49984/60000]\n",
            "loss: 762.029541  [50624/60000]\n",
            "loss: 2873.819336  [51264/60000]\n",
            "loss: 3344.338379  [51904/60000]\n",
            "loss: 4507.872559  [52544/60000]\n",
            "loss: 476.236328  [53184/60000]\n",
            "loss: 658.299438  [53824/60000]\n",
            "loss: 1509.018188  [54464/60000]\n",
            "loss: 746.158264  [55104/60000]\n",
            "loss: 751.211670  [55744/60000]\n",
            "loss: 551.436768  [56384/60000]\n",
            "loss: 1026.664551  [57024/60000]\n",
            "loss: 1493.977905  [57664/60000]\n",
            "loss: 496.357727  [58304/60000]\n",
            "loss: 718.321655  [58944/60000]\n",
            "loss: 2570.824463  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 66.2%, Avg loss: 2961.209015 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2341.316406  [   64/60000]\n",
            "loss: 692.576477  [  704/60000]\n",
            "loss: 345.067688  [ 1344/60000]\n",
            "loss: 469.354401  [ 1984/60000]\n",
            "loss: 1379.828369  [ 2624/60000]\n",
            "loss: 750.006470  [ 3264/60000]\n",
            "loss: 1231.398926  [ 3904/60000]\n",
            "loss: 558.406128  [ 4544/60000]\n",
            "loss: 3494.783447  [ 5184/60000]\n",
            "loss: 530.145935  [ 5824/60000]\n",
            "loss: 541.304932  [ 6464/60000]\n",
            "loss: 1477.479004  [ 7104/60000]\n",
            "loss: 2682.519043  [ 7744/60000]\n",
            "loss: 1224.564575  [ 8384/60000]\n",
            "loss: 1290.900391  [ 9024/60000]\n",
            "loss: 1952.602173  [ 9664/60000]\n",
            "loss: 1282.104492  [10304/60000]\n",
            "loss: 801.086914  [10944/60000]\n",
            "loss: 1264.688843  [11584/60000]\n",
            "loss: 1806.384033  [12224/60000]\n",
            "loss: 1632.371094  [12864/60000]\n",
            "loss: 1386.846436  [13504/60000]\n",
            "loss: 306.064911  [14144/60000]\n",
            "loss: 923.519226  [14784/60000]\n",
            "loss: 199.834717  [15424/60000]\n",
            "loss: 990.881958  [16064/60000]\n",
            "loss: 501.050659  [16704/60000]\n",
            "loss: 1356.329346  [17344/60000]\n",
            "loss: 1434.573975  [17984/60000]\n",
            "loss: 655.687073  [18624/60000]\n",
            "loss: 1016.400269  [19264/60000]\n",
            "loss: 1422.677246  [19904/60000]\n",
            "loss: 435.534241  [20544/60000]\n",
            "loss: 938.311035  [21184/60000]\n",
            "loss: 3593.242188  [21824/60000]\n",
            "loss: 1223.351318  [22464/60000]\n",
            "loss: 672.638916  [23104/60000]\n",
            "loss: 689.050354  [23744/60000]\n",
            "loss: 1046.561035  [24384/60000]\n",
            "loss: 830.720703  [25024/60000]\n",
            "loss: 1474.765991  [25664/60000]\n",
            "loss: 2084.280273  [26304/60000]\n",
            "loss: 1168.496338  [26944/60000]\n",
            "loss: 1753.718506  [27584/60000]\n",
            "loss: 586.987244  [28224/60000]\n",
            "loss: 248.706146  [28864/60000]\n",
            "loss: 1274.022949  [29504/60000]\n",
            "loss: 2316.093262  [30144/60000]\n",
            "loss: 956.839722  [30784/60000]\n",
            "loss: 759.707031  [31424/60000]\n",
            "loss: 2080.383789  [32064/60000]\n",
            "loss: 677.191528  [32704/60000]\n",
            "loss: 290.984161  [33344/60000]\n",
            "loss: 1006.987366  [33984/60000]\n",
            "loss: 307.833801  [34624/60000]\n",
            "loss: 398.906403  [35264/60000]\n",
            "loss: 897.615234  [35904/60000]\n",
            "loss: 836.978577  [36544/60000]\n",
            "loss: 1836.344849  [37184/60000]\n",
            "loss: 396.530914  [37824/60000]\n",
            "loss: 1439.201538  [38464/60000]\n",
            "loss: 1664.213867  [39104/60000]\n",
            "loss: 1386.644165  [39744/60000]\n",
            "loss: 2095.872559  [40384/60000]\n",
            "loss: 1472.131348  [41024/60000]\n",
            "loss: 1047.235718  [41664/60000]\n",
            "loss: 1006.084595  [42304/60000]\n",
            "loss: 1995.205811  [42944/60000]\n",
            "loss: 960.093872  [43584/60000]\n",
            "loss: 620.292725  [44224/60000]\n",
            "loss: 771.651794  [44864/60000]\n",
            "loss: 746.487000  [45504/60000]\n",
            "loss: 961.510742  [46144/60000]\n",
            "loss: 1158.081055  [46784/60000]\n",
            "loss: 390.275360  [47424/60000]\n",
            "loss: 952.215210  [48064/60000]\n",
            "loss: 708.044189  [48704/60000]\n",
            "loss: 834.012085  [49344/60000]\n",
            "loss: 1469.021484  [49984/60000]\n",
            "loss: 754.192383  [50624/60000]\n",
            "loss: 2223.598145  [51264/60000]\n",
            "loss: 2934.671631  [51904/60000]\n",
            "loss: 1848.441895  [52544/60000]\n",
            "loss: 680.278198  [53184/60000]\n",
            "loss: 769.060059  [53824/60000]\n",
            "loss: 1098.303101  [54464/60000]\n",
            "loss: 264.266174  [55104/60000]\n",
            "loss: 628.318909  [55744/60000]\n",
            "loss: 910.752625  [56384/60000]\n",
            "loss: 849.679871  [57024/60000]\n",
            "loss: 1383.621582  [57664/60000]\n",
            "loss: 305.594818  [58304/60000]\n",
            "loss: 674.471252  [58944/60000]\n",
            "loss: 2102.312500  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 2308.165877 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1758.256104  [   64/60000]\n",
            "loss: 663.340332  [  704/60000]\n",
            "loss: 512.820801  [ 1344/60000]\n",
            "loss: 741.926880  [ 1984/60000]\n",
            "loss: 805.786865  [ 2624/60000]\n",
            "loss: 861.520752  [ 3264/60000]\n",
            "loss: 1111.768311  [ 3904/60000]\n",
            "loss: 985.058472  [ 4544/60000]\n",
            "loss: 1661.412964  [ 5184/60000]\n",
            "loss: 427.877441  [ 5824/60000]\n",
            "loss: 449.057495  [ 6464/60000]\n",
            "loss: 1065.125610  [ 7104/60000]\n",
            "loss: 1247.977417  [ 7744/60000]\n",
            "loss: 1254.797363  [ 8384/60000]\n",
            "loss: 840.372070  [ 9024/60000]\n",
            "loss: 638.003052  [ 9664/60000]\n",
            "loss: 527.728638  [10304/60000]\n",
            "loss: 549.374939  [10944/60000]\n",
            "loss: 1343.484741  [11584/60000]\n",
            "loss: 2365.020508  [12224/60000]\n",
            "loss: 931.712036  [12864/60000]\n",
            "loss: 1430.419922  [13504/60000]\n",
            "loss: 501.832306  [14144/60000]\n",
            "loss: 1634.140869  [14784/60000]\n",
            "loss: 903.490967  [15424/60000]\n",
            "loss: 1558.544556  [16064/60000]\n",
            "loss: 889.727051  [16704/60000]\n",
            "loss: 1296.760132  [17344/60000]\n",
            "loss: 954.125977  [17984/60000]\n",
            "loss: 641.475647  [18624/60000]\n",
            "loss: 1411.048340  [19264/60000]\n",
            "loss: 1630.988037  [19904/60000]\n",
            "loss: 539.374329  [20544/60000]\n",
            "loss: 858.585938  [21184/60000]\n",
            "loss: 2908.045654  [21824/60000]\n",
            "loss: 644.112854  [22464/60000]\n",
            "loss: 1334.707153  [23104/60000]\n",
            "loss: 654.610779  [23744/60000]\n",
            "loss: 942.159424  [24384/60000]\n",
            "loss: 829.562988  [25024/60000]\n",
            "loss: 2651.302734  [25664/60000]\n",
            "loss: 1864.485474  [26304/60000]\n",
            "loss: 1107.421631  [26944/60000]\n",
            "loss: 1726.287109  [27584/60000]\n",
            "loss: 748.857422  [28224/60000]\n",
            "loss: 1446.831909  [28864/60000]\n",
            "loss: 1324.639648  [29504/60000]\n",
            "loss: 1555.181763  [30144/60000]\n",
            "loss: 643.392090  [30784/60000]\n",
            "loss: 268.160431  [31424/60000]\n",
            "loss: 1045.953003  [32064/60000]\n",
            "loss: 794.372253  [32704/60000]\n",
            "loss: 753.205200  [33344/60000]\n",
            "loss: 1043.235596  [33984/60000]\n",
            "loss: 491.138000  [34624/60000]\n",
            "loss: 346.490906  [35264/60000]\n",
            "loss: 1037.964722  [35904/60000]\n",
            "loss: 819.792297  [36544/60000]\n",
            "loss: 2517.685791  [37184/60000]\n",
            "loss: 629.015747  [37824/60000]\n",
            "loss: 1622.590332  [38464/60000]\n",
            "loss: 861.556274  [39104/60000]\n",
            "loss: 1073.923096  [39744/60000]\n",
            "loss: 975.671143  [40384/60000]\n",
            "loss: 627.590576  [41024/60000]\n",
            "loss: 1114.700073  [41664/60000]\n",
            "loss: 1018.763733  [42304/60000]\n",
            "loss: 584.575134  [42944/60000]\n",
            "loss: 1214.932861  [43584/60000]\n",
            "loss: 1791.121460  [44224/60000]\n",
            "loss: 1064.733765  [44864/60000]\n",
            "loss: 752.449036  [45504/60000]\n",
            "loss: 782.643982  [46144/60000]\n",
            "loss: 1236.083008  [46784/60000]\n",
            "loss: 1007.908752  [47424/60000]\n",
            "loss: 1065.491699  [48064/60000]\n",
            "loss: 641.624573  [48704/60000]\n",
            "loss: 663.172607  [49344/60000]\n",
            "loss: 1179.793457  [49984/60000]\n",
            "loss: 711.467773  [50624/60000]\n",
            "loss: 2231.098145  [51264/60000]\n",
            "loss: 2174.304932  [51904/60000]\n",
            "loss: 1847.012573  [52544/60000]\n",
            "loss: 1235.571777  [53184/60000]\n",
            "loss: 588.995300  [53824/60000]\n",
            "loss: 772.666748  [54464/60000]\n",
            "loss: 476.602905  [55104/60000]\n",
            "loss: 682.894165  [55744/60000]\n",
            "loss: 570.893982  [56384/60000]\n",
            "loss: 923.726807  [57024/60000]\n",
            "loss: 957.075073  [57664/60000]\n",
            "loss: 247.094574  [58304/60000]\n",
            "loss: 713.101807  [58944/60000]\n",
            "loss: 2019.595703  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.8%, Avg loss: 2173.638810 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1418.072998  [   64/60000]\n",
            "loss: 954.809937  [  704/60000]\n",
            "loss: 1713.009277  [ 1344/60000]\n",
            "loss: 1112.581665  [ 1984/60000]\n",
            "loss: 986.746277  [ 2624/60000]\n",
            "loss: 754.576294  [ 3264/60000]\n",
            "loss: 1188.663696  [ 3904/60000]\n",
            "loss: 670.551880  [ 4544/60000]\n",
            "loss: 925.496216  [ 5184/60000]\n",
            "loss: 485.269714  [ 5824/60000]\n",
            "loss: 786.329346  [ 6464/60000]\n",
            "loss: 1142.013184  [ 7104/60000]\n",
            "loss: 2474.026367  [ 7744/60000]\n",
            "loss: 1661.505737  [ 8384/60000]\n",
            "loss: 815.784180  [ 9024/60000]\n",
            "loss: 702.234497  [ 9664/60000]\n",
            "loss: 1129.955811  [10304/60000]\n",
            "loss: 2103.582764  [10944/60000]\n",
            "loss: 1366.951416  [11584/60000]\n",
            "loss: 1923.582031  [12224/60000]\n",
            "loss: 2906.677002  [12864/60000]\n",
            "loss: 992.869751  [13504/60000]\n",
            "loss: 355.675262  [14144/60000]\n",
            "loss: 865.937378  [14784/60000]\n",
            "loss: 350.261475  [15424/60000]\n",
            "loss: 1031.250000  [16064/60000]\n",
            "loss: 598.416443  [16704/60000]\n",
            "loss: 1494.465332  [17344/60000]\n",
            "loss: 1210.603638  [17984/60000]\n",
            "loss: 612.512939  [18624/60000]\n",
            "loss: 1173.421875  [19264/60000]\n",
            "loss: 1310.016479  [19904/60000]\n",
            "loss: 669.489197  [20544/60000]\n",
            "loss: 677.663086  [21184/60000]\n",
            "loss: 3552.322998  [21824/60000]\n",
            "loss: 1027.859985  [22464/60000]\n",
            "loss: 2037.630249  [23104/60000]\n",
            "loss: 779.406921  [23744/60000]\n",
            "loss: 1171.373901  [24384/60000]\n",
            "loss: 868.837769  [25024/60000]\n",
            "loss: 1396.536743  [25664/60000]\n",
            "loss: 1517.239502  [26304/60000]\n",
            "loss: 1253.942261  [26944/60000]\n",
            "loss: 1459.793213  [27584/60000]\n",
            "loss: 596.289429  [28224/60000]\n",
            "loss: 1594.809814  [28864/60000]\n",
            "loss: 1320.975342  [29504/60000]\n",
            "loss: 1405.970459  [30144/60000]\n",
            "loss: 1029.887695  [30784/60000]\n",
            "loss: 324.267517  [31424/60000]\n",
            "loss: 734.529724  [32064/60000]\n",
            "loss: 718.042664  [32704/60000]\n",
            "loss: 679.484375  [33344/60000]\n",
            "loss: 1418.834961  [33984/60000]\n",
            "loss: 476.285980  [34624/60000]\n",
            "loss: 502.119568  [35264/60000]\n",
            "loss: 878.963196  [35904/60000]\n",
            "loss: 886.404297  [36544/60000]\n",
            "loss: 2250.677002  [37184/60000]\n",
            "loss: 441.579041  [37824/60000]\n",
            "loss: 1249.725098  [38464/60000]\n",
            "loss: 610.768311  [39104/60000]\n",
            "loss: 1847.240234  [39744/60000]\n",
            "loss: 1036.431763  [40384/60000]\n",
            "loss: 1183.896484  [41024/60000]\n",
            "loss: 1769.154785  [41664/60000]\n",
            "loss: 1259.566772  [42304/60000]\n",
            "loss: 496.054382  [42944/60000]\n",
            "loss: 883.080322  [43584/60000]\n",
            "loss: 2430.647461  [44224/60000]\n",
            "loss: 1240.349609  [44864/60000]\n",
            "loss: 784.266418  [45504/60000]\n",
            "loss: 823.653564  [46144/60000]\n",
            "loss: 1271.483276  [46784/60000]\n",
            "loss: 1244.950928  [47424/60000]\n",
            "loss: 1248.890625  [48064/60000]\n",
            "loss: 917.543579  [48704/60000]\n",
            "loss: 667.673035  [49344/60000]\n",
            "loss: 1070.445068  [49984/60000]\n",
            "loss: 1033.311279  [50624/60000]\n",
            "loss: 2465.086914  [51264/60000]\n",
            "loss: 2452.295898  [51904/60000]\n",
            "loss: 2383.172119  [52544/60000]\n",
            "loss: 647.130249  [53184/60000]\n",
            "loss: 982.459473  [53824/60000]\n",
            "loss: 1563.226196  [54464/60000]\n",
            "loss: 321.791443  [55104/60000]\n",
            "loss: 1225.705322  [55744/60000]\n",
            "loss: 1235.957642  [56384/60000]\n",
            "loss: 1113.397949  [57024/60000]\n",
            "loss: 1472.032471  [57664/60000]\n",
            "loss: 406.626373  [58304/60000]\n",
            "loss: 695.090820  [58944/60000]\n",
            "loss: 1753.189209  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.2%, Avg loss: 2143.017959 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1521.207764  [   64/60000]\n",
            "loss: 1176.514160  [  704/60000]\n",
            "loss: 266.620972  [ 1344/60000]\n",
            "loss: 580.179016  [ 1984/60000]\n",
            "loss: 1015.674927  [ 2624/60000]\n",
            "loss: 1003.395813  [ 3264/60000]\n",
            "loss: 1165.430420  [ 3904/60000]\n",
            "loss: 1299.938354  [ 4544/60000]\n",
            "loss: 849.784668  [ 5184/60000]\n",
            "loss: 1387.489990  [ 5824/60000]\n",
            "loss: 599.581543  [ 6464/60000]\n",
            "loss: 1013.319458  [ 7104/60000]\n",
            "loss: 1174.802490  [ 7744/60000]\n",
            "loss: 1408.014282  [ 8384/60000]\n",
            "loss: 936.067566  [ 9024/60000]\n",
            "loss: 1019.798279  [ 9664/60000]\n",
            "loss: 739.546326  [10304/60000]\n",
            "loss: 886.985901  [10944/60000]\n",
            "loss: 980.345215  [11584/60000]\n",
            "loss: 2136.974854  [12224/60000]\n",
            "loss: 1671.816895  [12864/60000]\n",
            "loss: 1711.835571  [13504/60000]\n",
            "loss: 319.359406  [14144/60000]\n",
            "loss: 1751.012451  [14784/60000]\n",
            "loss: 606.726440  [15424/60000]\n",
            "loss: 1107.014160  [16064/60000]\n",
            "loss: 891.174194  [16704/60000]\n",
            "loss: 1223.910889  [17344/60000]\n",
            "loss: 828.989746  [17984/60000]\n",
            "loss: 581.628174  [18624/60000]\n",
            "loss: 1039.338135  [19264/60000]\n",
            "loss: 1894.734131  [19904/60000]\n",
            "loss: 514.363525  [20544/60000]\n",
            "loss: 1055.897095  [21184/60000]\n",
            "loss: 2519.090332  [21824/60000]\n",
            "loss: 1250.283569  [22464/60000]\n",
            "loss: 581.842102  [23104/60000]\n",
            "loss: 776.828369  [23744/60000]\n",
            "loss: 860.610474  [24384/60000]\n",
            "loss: 625.782654  [25024/60000]\n",
            "loss: 996.096497  [25664/60000]\n",
            "loss: 2057.476807  [26304/60000]\n",
            "loss: 1022.255371  [26944/60000]\n",
            "loss: 2114.417480  [27584/60000]\n",
            "loss: 579.968628  [28224/60000]\n",
            "loss: 1251.161377  [28864/60000]\n",
            "loss: 1120.675537  [29504/60000]\n",
            "loss: 1405.475098  [30144/60000]\n",
            "loss: 658.592957  [30784/60000]\n",
            "loss: 579.313965  [31424/60000]\n",
            "loss: 831.202148  [32064/60000]\n",
            "loss: 856.309082  [32704/60000]\n",
            "loss: 374.310486  [33344/60000]\n",
            "loss: 1154.777222  [33984/60000]\n",
            "loss: 450.920349  [34624/60000]\n",
            "loss: 492.397461  [35264/60000]\n",
            "loss: 1083.865112  [35904/60000]\n",
            "loss: 2104.901367  [36544/60000]\n",
            "loss: 1794.007080  [37184/60000]\n",
            "loss: 438.519653  [37824/60000]\n",
            "loss: 1179.208618  [38464/60000]\n",
            "loss: 596.939453  [39104/60000]\n",
            "loss: 1468.034302  [39744/60000]\n",
            "loss: 986.245667  [40384/60000]\n",
            "loss: 1456.896851  [41024/60000]\n",
            "loss: 938.327026  [41664/60000]\n",
            "loss: 1490.990845  [42304/60000]\n",
            "loss: 631.682068  [42944/60000]\n",
            "loss: 643.872253  [43584/60000]\n",
            "loss: 1578.124878  [44224/60000]\n",
            "loss: 789.913818  [44864/60000]\n",
            "loss: 699.584900  [45504/60000]\n",
            "loss: 928.270996  [46144/60000]\n",
            "loss: 1099.823364  [46784/60000]\n",
            "loss: 483.741302  [47424/60000]\n",
            "loss: 1245.957031  [48064/60000]\n",
            "loss: 1005.483643  [48704/60000]\n",
            "loss: 610.027588  [49344/60000]\n",
            "loss: 1490.034912  [49984/60000]\n",
            "loss: 1017.565002  [50624/60000]\n",
            "loss: 1330.137939  [51264/60000]\n",
            "loss: 3330.423584  [51904/60000]\n",
            "loss: 4234.446777  [52544/60000]\n",
            "loss: 1198.142578  [53184/60000]\n",
            "loss: 764.178406  [53824/60000]\n",
            "loss: 1851.534302  [54464/60000]\n",
            "loss: 625.581055  [55104/60000]\n",
            "loss: 831.924500  [55744/60000]\n",
            "loss: 882.830078  [56384/60000]\n",
            "loss: 1006.079407  [57024/60000]\n",
            "loss: 1647.561523  [57664/60000]\n",
            "loss: 314.606506  [58304/60000]\n",
            "loss: 752.442627  [58944/60000]\n",
            "loss: 2086.614990  [59584/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.7%, Avg loss: 2141.614471 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our model for later, so we can train more or make predictions\n",
        "\n",
        "EPOCH = epochs\n",
        "# We use the .pt file extension by convention for saving\n",
        "#    pytorch models\n",
        "PATH = \"model.pt\"\n",
        "\n",
        "# The save function creates a binary storing all our data for us\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, PATH)"
      ],
      "metadata": {
        "id": "Sb7p4C7poohU"
      },
      "id": "Sb7p4C7poohU",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify our path\n",
        "PATH = \"model.pt\"\n",
        "\n",
        "# Create a new \"blank\" model to load our information into\n",
        "model = FashionNet()\n",
        "\n",
        "# Recreate our optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Load back all of our data from the file\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "EPOCH = checkpoint['epoch']"
      ],
      "metadata": {
        "id": "OPtNFIqJp9Fi"
      },
      "id": "OPtNFIqJp9Fi",
      "execution_count": 29,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}